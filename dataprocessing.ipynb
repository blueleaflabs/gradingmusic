{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIRST PASS: Downloading from YouTube for rows with Processed = -1 ===\n",
      "Found 0 YouTube URLs to download (Processed=-1).\n",
      "=== SECOND PASS: Chunking files for rows with Processed = 0 ===\n",
      "Found 1104 files marked Processed=0 in metadata for chunking.\n",
      "Error: File '.DS_Store_2' is not .mp3 or .wav. Skipping.\n",
      "Appended 43959 rows to 'outputchunks' in second pass.\n",
      "Updated 1104 rows in 'metadata' after chunking pass.\n",
      "All passes completed.\n"
     ]
    }
   ],
   "source": [
    "# If not installed, you may need:\n",
    "# !pip install pydub gspread oauth2client yt-dlp\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Google Sheets\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "##################################################\n",
    "# A) GOOGLE SHEET CONFIG & AUTH\n",
    "##################################################\n",
    "\n",
    "SERVICE_ACCOUNT_JSON = os.path.join(\"data\", \"quantummusic-8dbce27ed321.json\")\n",
    "SCOPE = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_JSON, scopes=SCOPE)\n",
    "gc = gspread.authorize(creds)\n",
    "\n",
    "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1oD1Oyp5inbcog9eUQTl8xnmYi6y-ZMln8Iaxz9xHEtY/edit#gid=780967666\"\n",
    "workbook = gc.open_by_url(SPREADSHEET_URL)\n",
    "\n",
    "metadata_sheet       = workbook.worksheet(\"metadata\")\n",
    "tempogroups_sheet    = workbook.worksheet(\"tempogroups\")\n",
    "outputchunks_sheet   = workbook.worksheet(\"outputchunks\")\n",
    "\n",
    "##################################################\n",
    "# B) HELPER: Find column index by name\n",
    "##################################################\n",
    "def find_col_index(col_name, header_list):\n",
    "    \"\"\"\n",
    "    Locate column index (0-based) for 'col_name' in 'header_list' (case-insensitive).\n",
    "    Returns None if not found.\n",
    "    \"\"\"\n",
    "    col_name_lower = col_name.lower()\n",
    "    for i, h in enumerate(header_list):\n",
    "        if h.lower() == col_name_lower:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "\n",
    "##################################################\n",
    "# C) FIRST PASS: Download from YouTube if Processed = -1\n",
    "##################################################\n",
    "def first_pass_downloads():\n",
    "    print(\"=== FIRST PASS: Downloading from YouTube for rows with Processed = -1 ===\")\n",
    "\n",
    "    # 1) Read entire \"metadata\" sheet\n",
    "    all_values = metadata_sheet.get_all_values()\n",
    "    if not all_values:\n",
    "        print(\"No data in 'metadata' sheet. Nothing to do.\")\n",
    "        return  # no data\n",
    "\n",
    "    header = all_values[0]\n",
    "    data_rows = all_values[1:]\n",
    "\n",
    "    # Identify relevant columns\n",
    "    tracksource_idx = find_col_index(\"TrackSource\", header)\n",
    "    processed_idx   = find_col_index(\"Processed\", header)\n",
    "    filename_idx    = find_col_index(\"FileName\", header)\n",
    "\n",
    "    if tracksource_idx is None or processed_idx is None or filename_idx is None:\n",
    "        raise ValueError(\"Could not find 'TrackSource', 'FileName', or 'Processed' columns in metadata header.\")\n",
    "\n",
    "    # 2) Collect rows where Processed = -1\n",
    "    rows_to_download = []\n",
    "    for i, row in enumerate(data_rows):\n",
    "        sheet_row = i + 2  # 1-based offset for header\n",
    "        if len(row) <= max(tracksource_idx, processed_idx, filename_idx):\n",
    "            continue\n",
    "        \n",
    "        tracksource_val = row[tracksource_idx].strip()\n",
    "        processed_val   = row[processed_idx].strip()\n",
    "        file_name_in_sheet = row[filename_idx].strip()\n",
    "\n",
    "        if processed_val == \"-1\" and tracksource_val:\n",
    "            rows_to_download.append((sheet_row, tracksource_val, file_name_in_sheet))\n",
    "\n",
    "    print(f\"Found {len(rows_to_download)} YouTube URLs to download (Processed=-1).\")\n",
    "\n",
    "    download_folder = os.path.join(\"data\", \"rawunprocessed\")\n",
    "    os.makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "    # We'll store final statuses in a dict: row -> new_processed_val\n",
    "    status_updates = {}\n",
    "\n",
    "    for (sheet_row_number, youtube_url, file_name_in_sheet) in rows_to_download:\n",
    "        print(f\"Downloading row {sheet_row_number}, URL: {youtube_url}\")\n",
    "        \n",
    "        # Ensure the final filename ends with .mp3\n",
    "        # (If user already put .mp3, we won't double-append it.)\n",
    "        if not file_name_in_sheet.lower().endswith(\".mp3\"):\n",
    "            file_name_in_sheet += \".mp3\"\n",
    "\n",
    "        # Full path for output\n",
    "        output_path = os.path.join(download_folder, file_name_in_sheet)\n",
    "\n",
    "        cmd = [\n",
    "            \"yt-dlp\",\n",
    "            \"-f\", \"bestaudio/best\",\n",
    "            \"--extract-audio\",\n",
    "            \"--audio-format\", \"mp3\",\n",
    "            \"--audio-quality\", \"320K\",\n",
    "            \"-o\", output_path,\n",
    "            youtube_url\n",
    "        ]\n",
    "        try:\n",
    "            result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"Download succeeded.\")\n",
    "                # Mark processed=0 so it can be chunked in second pass\n",
    "                status_updates[sheet_row_number] = \"11\"\n",
    "            else:\n",
    "                print(f\"Download error (returncode={result.returncode}): {result.stderr}\")\n",
    "                # Mark processed=-2 to indicate error\n",
    "                status_updates[sheet_row_number] = \"-2\"\n",
    "        except Exception as e:\n",
    "            print(f\"Exception in download: {e}\")\n",
    "            status_updates[sheet_row_number] = \"-2\"\n",
    "        \n",
    "        # Wait 10 seconds before next\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # 3) Batch update sheet for these rows\n",
    "    if status_updates:\n",
    "        col_number = processed_idx + 1  # 1-based for Sheets\n",
    "        cell_updates = []\n",
    "        for row_idx, new_val in status_updates.items():\n",
    "            cell_updates.append({\n",
    "                'range': f\"R{row_idx}C{col_number}\",\n",
    "                'values': [[new_val]]\n",
    "            })\n",
    "        metadata_sheet.batch_update(cell_updates)\n",
    "        print(f\"Updated {len(cell_updates)} rows in 'metadata' after YouTube downloads.\")\n",
    "\n",
    "\n",
    "##################################################\n",
    "# D) SECOND PASS: Chunk if Processed = 0\n",
    "##################################################\n",
    "\n",
    "def second_pass_chunking():\n",
    "    print(\"=== SECOND PASS: Chunking files for rows with Processed = 0 ===\")\n",
    "\n",
    "    # 1) Re-read \"metadata\" to see the updated states\n",
    "    metadata_all_values = metadata_sheet.get_all_values()\n",
    "    if not metadata_all_values:\n",
    "        print(\"No data in 'metadata' sheet.\")\n",
    "        return\n",
    "\n",
    "    metadata_header = metadata_all_values[0]\n",
    "    metadata_data_rows = metadata_all_values[1:] if len(metadata_all_values) > 1 else []\n",
    "\n",
    "    file_col_idx = find_col_index(\"FileName\", metadata_header)\n",
    "    processed_col_idx = find_col_index(\"Processed\", metadata_header)\n",
    "    if file_col_idx is None or processed_col_idx is None:\n",
    "        raise ValueError(\"Could not find 'FileName' or 'Processed' column in metadata header.\")\n",
    "\n",
    "    # Build list of (sheet_row_number, file_name_in_sheet) where processed=0\n",
    "    unprocessed_entries = []\n",
    "    for i, row in enumerate(metadata_data_rows):\n",
    "        sheet_row_number = i + 2\n",
    "        if len(row) <= max(file_col_idx, processed_col_idx):\n",
    "            continue\n",
    "        \n",
    "        file_name_in_sheet = row[file_col_idx].strip()\n",
    "        processed_val = row[processed_col_idx].strip()\n",
    "        if processed_val == \"0\":\n",
    "            unprocessed_entries.append((sheet_row_number, file_name_in_sheet))\n",
    "\n",
    "    print(f\"Found {len(unprocessed_entries)} files marked Processed=0 in metadata for chunking.\")\n",
    "\n",
    "    # 2) Build set of base names for these\n",
    "    unprocessed_base_names = set()\n",
    "    for _, file_name_in_sheet in unprocessed_entries:\n",
    "        base_name = os.path.splitext(file_name_in_sheet)[0]\n",
    "        unprocessed_base_names.add(base_name)\n",
    "\n",
    "    # 3) Read tempogroups, store only for these base names\n",
    "    tempogroups_all = tempogroups_sheet.get_all_records()\n",
    "    tempo_dict = {}\n",
    "    for row in tempogroups_all:\n",
    "        fname = row.get(\"FileName\", \"\").strip()\n",
    "        base_ = os.path.splitext(fname)[0]\n",
    "        if base_ not in unprocessed_base_names:\n",
    "            continue\n",
    "        start_dur = float(row.get(\"StartDuration\", 0))\n",
    "        end_dur   = float(row.get(\"EndDuration\", 0))\n",
    "        tempo     = str(row.get(\"Tempo\", \"Unknown\"))\n",
    "        if base_ not in tempo_dict:\n",
    "            tempo_dict[base_] = []\n",
    "        tempo_dict[base_].append({\n",
    "            \"start\": start_dur,\n",
    "            \"end\": end_dur,\n",
    "            \"tempo\": tempo\n",
    "        })\n",
    "\n",
    "    # Folders & settings\n",
    "    data_folder      = \"data\"\n",
    "    input_folder     = os.path.join(data_folder, \"rawunprocessed\")\n",
    "    output_folder    = os.path.join(data_folder, \"trainingdata\")\n",
    "    processed_folder = os.path.join(data_folder, \"rawprocessed\")\n",
    "    error_folder     = os.path.join(data_folder, \"rawerror\")\n",
    "\n",
    "    target_sample_rate = 44100\n",
    "    chunk_duration_sec = 20\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    os.makedirs(processed_folder, exist_ok=True)\n",
    "    os.makedirs(error_folder, exist_ok=True)\n",
    "\n",
    "    # Helper functions\n",
    "    def process_audio_file(file_path, is_mp3):\n",
    "        \"\"\"Load MP3 or WAV, convert to target SR mono.\"\"\"\n",
    "        if is_mp3:\n",
    "            audio = AudioSegment.from_file(file_path, format=\"mp3\")\n",
    "        else:\n",
    "            audio = AudioSegment.from_file(file_path, format=\"wav\")\n",
    "        audio = audio.set_frame_rate(target_sample_rate).set_channels(1)\n",
    "        duration_sec = len(audio) / 1000.0\n",
    "        return audio, duration_sec\n",
    "\n",
    "    def chunk_audio_by_tempo(audio, base_name, file_name_sheet, tempo_info):\n",
    "        rows_for_sheet = []\n",
    "        for tinfo in tempo_info:\n",
    "            tempo_start = float(tinfo[\"start\"])\n",
    "            tempo_end   = float(tinfo[\"end\"])\n",
    "            tempo_label = tinfo[\"tempo\"]\n",
    "            chunk_start_sec = tempo_start\n",
    "\n",
    "            while chunk_start_sec < tempo_end:\n",
    "                chunk_end_sec = chunk_start_sec + chunk_duration_sec\n",
    "                if chunk_end_sec > tempo_end:\n",
    "                    actual_length_sec = tempo_end - chunk_start_sec\n",
    "                    padded_sec = chunk_duration_sec - actual_length_sec\n",
    "                    start_ms = int(chunk_start_sec * 1000)\n",
    "                    end_ms   = int(tempo_end * 1000)\n",
    "                    audio_chunk = audio[start_ms:end_ms]\n",
    "                    if padded_sec > 0:\n",
    "                        silence_chunk = AudioSegment.silent(duration=int(padded_sec * 1000))\n",
    "                        audio_chunk = audio_chunk + silence_chunk\n",
    "\n",
    "                    chunk_filename = f\"{base_name}_{int(chunk_start_sec)}s_{int(chunk_end_sec)}s_{tempo_label}_padded.wav\"\n",
    "                    padded_flag = \"Y\"\n",
    "                else:\n",
    "                    start_ms = int(chunk_start_sec * 1000)\n",
    "                    end_ms   = int(chunk_end_sec * 1000)\n",
    "                    audio_chunk = audio[start_ms:end_ms]\n",
    "                    chunk_filename = f\"{base_name}_{int(chunk_start_sec)}s_{int(chunk_end_sec)}s_{tempo_label}.wav\"\n",
    "                    padded_flag = \"N\"\n",
    "\n",
    "                out_path = os.path.join(output_folder, chunk_filename)\n",
    "                audio_chunk.export(out_path, format=\"wav\")\n",
    "\n",
    "                rows_for_sheet.append({\n",
    "                    \"FileName\": file_name_sheet,\n",
    "                    \"ChunkFileName\": chunk_filename,\n",
    "                    \"Start Duration\": f\"{chunk_start_sec:.2f}\",\n",
    "                    \"End Duration\": f\"{min(chunk_end_sec, tempo_end):.2f}\",\n",
    "                    \"Tempo\": tempo_label,\n",
    "                    \"Padded\": padded_flag\n",
    "                })\n",
    "\n",
    "                chunk_start_sec += chunk_duration_sec\n",
    "\n",
    "        return rows_for_sheet\n",
    "\n",
    "    all_output_rows = []\n",
    "    status_updates_for_chunk = {}\n",
    "\n",
    "    # 4) Loop over entries with Processed=0\n",
    "    for (sheet_row_number, file_name_in_sheet) in unprocessed_entries:\n",
    "        local_path = os.path.join(input_folder, file_name_in_sheet)\n",
    "        lower_name = file_name_in_sheet.lower()\n",
    "\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"Error: File '{file_name_in_sheet}' not found in rawunprocessed. Skipping.\")\n",
    "            # Mark processed = -3\n",
    "            status_updates_for_chunk[sheet_row_number] = \"-3\"\n",
    "            continue\n",
    "\n",
    "        if not (lower_name.endswith(\".mp3\") or lower_name.endswith(\".wav\")):\n",
    "            print(f\"Error: File '{file_name_in_sheet}' is not .mp3 or .wav. Skipping.\")\n",
    "            status_updates_for_chunk[sheet_row_number] = \"-3\"\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            is_mp3 = lower_name.endswith(\".mp3\")\n",
    "            audio, total_sec = process_audio_file(local_path, is_mp3)\n",
    "            base_name = os.path.splitext(file_name_in_sheet)[0]\n",
    "            if base_name in tempo_dict and tempo_dict[base_name]:\n",
    "                tempo_info = tempo_dict[base_name]\n",
    "            else:\n",
    "                tempo_info = [{\n",
    "                    \"start\": 0,\n",
    "                    \"end\": total_sec,\n",
    "                    \"tempo\": \"Unknown\"\n",
    "                }]\n",
    "\n",
    "            chunk_rows = chunk_audio_by_tempo(audio, base_name, file_name_in_sheet, tempo_info)\n",
    "            all_output_rows.extend(chunk_rows)\n",
    "\n",
    "            # Move original file to processed\n",
    "            shutil.move(local_path, os.path.join(processed_folder, file_name_in_sheet))\n",
    "            status_updates_for_chunk[sheet_row_number] = \"1\"  # success\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name_in_sheet}: {e}\")\n",
    "            if os.path.exists(local_path):\n",
    "                shutil.move(local_path, os.path.join(error_folder, file_name_in_sheet))\n",
    "            status_updates_for_chunk[sheet_row_number] = \"-3\"\n",
    "\n",
    "    # 5) Single append to 'outputchunks'\n",
    "    if all_output_rows:\n",
    "        rows_to_append = []\n",
    "        for row in all_output_rows:\n",
    "            rows_to_append.append([\n",
    "                row[\"FileName\"],\n",
    "                row[\"ChunkFileName\"],\n",
    "                row[\"Start Duration\"],\n",
    "                row[\"End Duration\"],\n",
    "                row[\"Tempo\"],\n",
    "                row[\"Padded\"]\n",
    "            ])\n",
    "        outputchunks_sheet.append_rows(rows_to_append, value_input_option='RAW')\n",
    "        print(f\"Appended {len(rows_to_append)} rows to 'outputchunks' in second pass.\")\n",
    "\n",
    "    # 6) Batch update \"Processed\" for chunk results\n",
    "    if status_updates_for_chunk:\n",
    "        col_number = processed_col_idx + 1\n",
    "        cell_updates = []\n",
    "        for row_idx, new_val in status_updates_for_chunk.items():\n",
    "            cell_updates.append({\n",
    "                'range': f\"R{row_idx}C{col_number}\",\n",
    "                'values': [[new_val]]\n",
    "            })\n",
    "        metadata_sheet.batch_update(cell_updates)\n",
    "        print(f\"Updated {len(cell_updates)} rows in 'metadata' after chunking pass.\")\n",
    "\n",
    "\n",
    "##################################################\n",
    "# E) SINGLE CELL MAIN LOGIC\n",
    "##################################################\n",
    "\n",
    "def run_all_passes():\n",
    "    \"\"\"\n",
    "    1) First pass: download from YouTube where Processed=-1, set them to 0 or -2\n",
    "    2) Second pass: chunk audio where Processed=0, set them to 1 or -3\n",
    "    \"\"\"\n",
    "    first_pass_downloads()\n",
    "    second_pass_chunking()\n",
    "    print(\"All passes completed.\")\n",
    "\n",
    "# Finally, just call run_all_passes():\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_passes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
