{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Full audio analysis pipeline that:\n",
    "  - Defines all relevant constants exactly as in your BBEdit file.\n",
    "  - Preprocesses audio (load, resample, normalize, trim, bandpass, HPSS).\n",
    "  - Computes spectral data, pitch, Praat metrics, RMS, LUFS, etc.\n",
    "  - Builds the final JSON structure (time_matrices, summary, advanced features).\n",
    "  - Inserts the result into PostgreSQL via QuantumMusicDB (save_to_db).\n",
    "  - Provides a main function grade_single_file that orchestrates everything.\n",
    "\n",
    "Updated to:\n",
    "  - Use `librosa.feature.rhythm.tempo` instead of `librosa.beat.tempo`.\n",
    "  - Zero-pad short chunks to avoid \"n_fft too large\" warnings.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "import shutil\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import colormaps\n",
    "from scipy.signal import find_peaks, iirnotch, filtfilt, butter\n",
    "import pyloudnorm as pyln\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import AerSimulator\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.visualization import plot_histogram\n",
    "import optuna\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import pyloudnorm as pyln  # ensure installed, otherwise LOUDNORM_AVAILABLE = False\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Database\n",
    "import psycopg2\n",
    "from psycopg2.extras import Json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============== CONSTANTS & IMPORTS from your BBEdit snippet =============\n",
    "\n",
    "INPUT_DIR = \"data/trainingdata\"\n",
    "OUTPUT_DIR = \"data/analysisoutput\"\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "DB_NAME = \"quantummusic_csef\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_USER = \"postgres\"  # placeholder\n",
    "DB_PASSWORD = \"postgres\"  # placeholder\n",
    "\n",
    "# Audio processing constants\n",
    "STANDARD_SR = 44100  # Standard sampling rate\n",
    "SILENCE_THRESHOLD_DB = 30  # dB threshold for silence trimming\n",
    "\n",
    "# Band-pass filter constants\n",
    "LOW_FREQ = 80.0\n",
    "HIGH_FREQ = 3000.0\n",
    "\n",
    "FIG_SIZE = (10, 4)\n",
    "\n",
    "SAVE_TO_DB = True\n",
    "N_MFCC = 40\n",
    "\n",
    "# Frame-based approach for pitch detection\n",
    "FRAME_SIZE = 2048\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "# Praat-based chunk size\n",
    "PRAAT_CHUNK_SIZE = 2048\n",
    "\n",
    "# Deviation threshold in cents, for dev_flag\n",
    "DEVIATION_THRESHOLD = 50.0\n",
    "\n",
    "HPSS_MARGIN = (1.0, 3.0)\n",
    "BANDPASS_FILTER_ORDER = 4\n",
    "\n",
    "# Multi-chunk tempo analysis\n",
    "TEMPO_CHUNK_SIZE_MEDIUM = 4096\n",
    "TEMPO_CHUNK_SIZE_LARGE = 22050\n",
    "\n",
    "#  constants for advanced vocal feature extraction\n",
    "VOCAL_FEATURE_CHUNK_SIZE = 22050\n",
    "VOCAL_FEATURE_CHUNK_HOP = 4096\n",
    "# keeping these the same as large chunk tempo analysis \n",
    "\n",
    "#  constant for LUFS calculations (0.5s @ 44.1kHz)\n",
    "LUFS_CHUNK_SIZE = 22050\n",
    "\n",
    "# ---  CONSTANTS FOR ADVANCED VOCAL FEATURE EXTRACTION ---\n",
    "# Formant analysis parameters\n",
    "FORMANT_ANALYSIS_TIME = 0.1\n",
    "FORMANT_TIME_STEP = 0.01\n",
    "MAX_NUMBER_OF_FORMANTS = 5\n",
    "MAXIMUM_FORMANT_FREQUENCY = 5500\n",
    "NUM_FORMANTS_TO_EXTRACT = 3\n",
    "\n",
    "# Pitch / jitter / shimmer parameters\n",
    "MIN_F0 = 75\n",
    "MAX_F0 = 500\n",
    "JITTER_TIME_STEP = 0.0001\n",
    "JITTER_MIN_PERIOD = 0.02\n",
    "JITTER_MAX_PERIOD = 1.3\n",
    "SHIMMER_MIN_AMPLITUDE = 0.0001\n",
    "SHIMMER_MAX_AMPLITUDE = 0.02\n",
    "SHIMMER_FACTOR = 1.6\n",
    "\n",
    "# Vibrato analysis parameters\n",
    "VIBRATO_MIN_HZ = 3\n",
    "VIBRATO_MAX_HZ = 10\n",
    "MEDIAN_FILTER_KERNEL_SIZE = 9\n",
    "\n",
    "\n",
    "# ============== Database Class =============\n",
    "class QuantumMusicDB:\n",
    "    \"\"\"\n",
    "    Connection to the PostgreSQL database and basic CRUD operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, db_name=DB_NAME, host=DB_HOST, user=DB_USER, password=DB_PASSWORD):\n",
    "        self.db_name = db_name\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(\n",
    "                dbname=self.db_name,\n",
    "                host=self.host,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            #print(f\"Connected to database {self.db_name} successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def create_tables(self):\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS audio_analysis (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            file_name VARCHAR(255),\n",
    "            upload_date TIMESTAMP DEFAULT NOW(),\n",
    "            sample_rate INT,\n",
    "            analysis_data JSONB\n",
    "        );\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(create_table_query)\n",
    "            self.conn.commit()\n",
    "        #print(\"Tables ensured.\")\n",
    "\n",
    "    def insert_analysis(self, file_name, sample_rate, analysis_data):\n",
    "        \"\"\"\n",
    "        Insert an analysis record into the DB.\n",
    "        analysis_data is stored as a JSONB column using psycopg2.extras.Json.\n",
    "        \"\"\"\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO audio_analysis(file_name, sample_rate, analysis_data)\n",
    "        VALUES (%s, %s, %s)\n",
    "        RETURNING id;\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(insert_query, (file_name, sample_rate, Json(analysis_data)))\n",
    "            _id = cur.fetchone()[0]\n",
    "            self.conn.commit()\n",
    "        return _id\n",
    "\n",
    "    def fetch_analysis(self, record_id):\n",
    "        \"\"\"\n",
    "        Fetch a specific analysis record by ID.\n",
    "        \"\"\"\n",
    "        select_query = \"\"\"\n",
    "        SELECT id, file_name, sample_rate, analysis_data\n",
    "        FROM audio_analysis\n",
    "        WHERE id=%s;\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            cur.execute(select_query, (record_id,))\n",
    "            row = cur.fetchone()\n",
    "        return row\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "\n",
    "# ============== Preprocessing =============\n",
    "def preprocess_audio(\n",
    "    file_path,\n",
    "    target_sr=STANDARD_SR,\n",
    "    silence_threshold_db=SILENCE_THRESHOLD_DB,\n",
    "    low_freq=LOW_FREQ,\n",
    "    high_freq=HIGH_FREQ,\n",
    "    margin=HPSS_MARGIN\n",
    "):\n",
    "    \"\"\"\n",
    "    Load audio, resample, normalize, trim silence, apply bandpass filter,\n",
    "    and remove percussive components (HPSS) in one function.\n",
    "    \"\"\"\n",
    "    audio_data, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "\n",
    "    # Normalize\n",
    "    peak = np.max(np.abs(audio_data))\n",
    "    if peak > 1e-9:\n",
    "        audio_data /= peak\n",
    "\n",
    "    # Trim silence\n",
    "    audio_data, _ = librosa.effects.trim(audio_data, top_db=silence_threshold_db)\n",
    "\n",
    "    # Bandpass filter\n",
    "    nyquist = 0.5 * sr\n",
    "    low = low_freq / nyquist\n",
    "    high = high_freq / nyquist\n",
    "    b, a = butter(N=BANDPASS_FILTER_ORDER, Wn=[low, high], btype='band')\n",
    "    audio_data = filtfilt(b, a, audio_data)\n",
    "\n",
    "    # HPSS\n",
    "    stft_data = librosa.stft(audio_data)\n",
    "    harmonic_part, _ = librosa.decompose.hpss(stft_data, margin=margin)\n",
    "    processed_audio = librosa.istft(harmonic_part)\n",
    "\n",
    "    return processed_audio, sr\n",
    "\n",
    "\n",
    "# ============== Utility to Avoid NaN in JSON =============\n",
    "def _replace_nan_with_none(value):\n",
    "    if isinstance(value, float) and np.isnan(value):\n",
    "        return None\n",
    "    elif isinstance(value, list):\n",
    "        return [_replace_nan_with_none(v) for v in value]\n",
    "    elif isinstance(value, tuple):\n",
    "        return tuple(_replace_nan_with_none(v) for v in value)\n",
    "    elif isinstance(value, dict):\n",
    "        return {k: _replace_nan_with_none(v) for k, v in value.items()}\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "\n",
    "# ============== Base pitch detection =============\n",
    "def detect_drone_pitch(audio_data, sr, min_freq=70.0, max_freq=300.0):\n",
    "    \"\"\"\n",
    "    Attempt to find a continuous drone by computing the time-averaged magnitude spectrum\n",
    "    and looking for the strongest peak in [min_freq..max_freq].\n",
    "    \n",
    "    If a peak is found above some dominance threshold, return that freq; else 0.0\n",
    "    \"\"\"\n",
    "    # We'll do a single STFT for the entire file.\n",
    "    # Zero-pad if audio is shorter than FRAME_SIZE to avoid 'n_fft too large' warnings.\n",
    "    n_fft = FRAME_SIZE  # references your existing constant\n",
    "    if len(audio_data) < n_fft:\n",
    "        pad_len = n_fft - len(audio_data)\n",
    "        audio_data = np.pad(audio_data, (0, pad_len), mode='constant')\n",
    "\n",
    "    # Compute STFT\n",
    "    S_complex = librosa.stft(audio_data, n_fft=n_fft, hop_length=HOP_LENGTH)\n",
    "    S_mag = np.abs(S_complex)\n",
    "\n",
    "    # Average the magnitude across time -> a single \"long-term\" spectrum\n",
    "    mean_spectrum = np.mean(S_mag, axis=1)  # shape => (n_fft/2+1,)\n",
    "\n",
    "    # Frequency array for each bin\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "\n",
    "    # Restrict analysis to [min_freq..max_freq]\n",
    "    valid_indices = np.where((freqs >= min_freq) & (freqs <= max_freq))[0]\n",
    "    if len(valid_indices) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sub_spectrum = mean_spectrum[valid_indices]\n",
    "    sub_freqs = freqs[valid_indices]\n",
    "\n",
    "    # Find the index of the maximum amplitude within that sub-range\n",
    "    max_idx = np.argmax(sub_spectrum)\n",
    "    peak_freq = sub_freqs[max_idx]\n",
    "    peak_val = sub_spectrum[max_idx]\n",
    "\n",
    "    # We do a quick \"dominance\" check: compare the loudest bin to the second-loudest\n",
    "    sorted_vals = np.sort(sub_spectrum)\n",
    "    if len(sorted_vals) > 1:\n",
    "        second_loudest = sorted_vals[-2]\n",
    "    else:\n",
    "        second_loudest = 1e-12\n",
    "\n",
    "    ratio = peak_val / (second_loudest + 1e-12)\n",
    "    # If ratio < ~1.5, we consider the peak not dominant enough to be a drone\n",
    "    if ratio < 1.5:\n",
    "        return 0.0\n",
    "\n",
    "    return float(peak_freq)\n",
    "\n",
    "\n",
    "def detect_base_pitch_with_pyin(audio_data, sr, fmin=MIN_F0, fmax=MAX_F0):\n",
    "    \"\"\"\n",
    "    If no drone, we fallback to median pitch from pyin across entire audio.\n",
    "    Returns 0.0 if no pitched frames are found.\n",
    "    \"\"\"\n",
    "    all_pitches, voiced_flags, _ = librosa.pyin(\n",
    "        y=audio_data,\n",
    "        sr=sr,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax\n",
    "    )\n",
    "    valid_pitches = []\n",
    "    for p, vf in zip(all_pitches, voiced_flags):\n",
    "        if vf and p is not None and not np.isnan(p):\n",
    "            valid_pitches.append(p)\n",
    "    if not valid_pitches:\n",
    "        return 0.0\n",
    "    return float(np.median(valid_pitches))\n",
    "\n",
    "\n",
    "\n",
    "# ============== Tone to Noise Calculation =============\n",
    "def compute_tone_to_noise(S_mag_column):\n",
    "    \"\"\"\n",
    "    Use spectral flatness to approximate tone_to_noise.\n",
    "    S_mag_column: magnitude for a single frame => shape (n_fft/2+1,).\n",
    "    \"\"\"\n",
    "    if len(S_mag_column) < 1:\n",
    "        return 0.0\n",
    "    numerator = np.exp(np.mean(np.log(S_mag_column + 1e-12)))\n",
    "    denominator = np.mean(S_mag_column + 1e-12)\n",
    "    flatness = numerator / denominator\n",
    "    return float(flatness)\n",
    "\n",
    "\n",
    "# ============== Transition Score Calculation =============\n",
    "def compute_transition_score(pitch_prev, pitch_curr):\n",
    "    \"\"\"\n",
    "    Example: difference-based measure. The smaller the difference, the bigger the score.\n",
    "    transition_score = 1.0 - min(|pitch_curr - pitch_prev| / 100.0, 1.0)\n",
    "    \"\"\"\n",
    "    if pitch_prev < 1e-6 or pitch_curr < 1e-6:\n",
    "        return 0.0\n",
    "    diff = abs(pitch_curr - pitch_prev)\n",
    "    raw_score = 1.0 - min(diff / 100.0, 1.0)\n",
    "    return max(raw_score, 0.0)\n",
    "\n",
    "\n",
    "# ============== Pitch -> Sruti Mapping =============\n",
    "def map_pitch_to_sruti(pitch_hz, base_pitch=240.0):\n",
    "    \"\"\"\n",
    "    Return (sruti_class, note_name, note_freq_hz, deviation_cents, dev_flag).\n",
    "    \"\"\"\n",
    "    if pitch_hz <= 0.0 or np.isnan(pitch_hz):\n",
    "        return (\"sruti_unknown\", \"unknown_note\", 0.0, 0.0, 0)\n",
    "    sruti_class = \"sruti_3\"  # dummy\n",
    "    note_name = \"Sa\"         # dummy\n",
    "    note_freq_hz = base_pitch\n",
    "    deviation_cents = (pitch_hz - base_pitch) * 10.0\n",
    "    dev_flag = 1 if abs(deviation_cents) > DEVIATION_THRESHOLD else 0\n",
    "    return (sruti_class, note_name, note_freq_hz, deviation_cents, dev_flag)\n",
    "\n",
    "\n",
    "# ============== Praat-based metrics per short chunk =============\n",
    "def compute_praat_metrics(chunk_data, sr):\n",
    "    \"\"\"\n",
    "    Return a dict with:\n",
    "      praat_hnr, hnr_category, jitter, shimmer,\n",
    "      formants = {F1, F2, F3},\n",
    "      vibrato_extent, vibrato_rate\n",
    "\n",
    "    Now with a real vibrato calculation:\n",
    "      - We extract pitch contour from parselmouth\n",
    "      - Convert pitch to cents around the median\n",
    "      - Band-limit the pitch contour to [3..10 Hz]\n",
    "      - Estimate vibrato_extent from the amplitude of that band\n",
    "      - Estimate vibrato_rate from the frequency peak in [3..10 Hz]\n",
    "    \"\"\"\n",
    "    out = {\n",
    "        \"praat_hnr\": 0.0,\n",
    "        \"hnr_category\": \"unknown\",\n",
    "        \"jitter\": 0.0,\n",
    "        \"shimmer\": 0.0,\n",
    "        \"formants\": {\"F1\": 0.0, \"F2\": 0.0, \"F3\": 0.0},\n",
    "        \"vibrato_extent\": 0.0,\n",
    "        \"vibrato_rate\": 0.0\n",
    "    }\n",
    "    # If chunk is too tiny, skip\n",
    "    if len(chunk_data) < 10:\n",
    "        return out\n",
    "\n",
    "    try:\n",
    "        import math\n",
    "        import numpy as np\n",
    "        import parselmouth\n",
    "        from parselmouth.praat import call\n",
    "        from scipy.signal import welch\n",
    "\n",
    "        snd = parselmouth.Sound(chunk_data, sr)\n",
    "\n",
    "        # 1) Pitch-based metrics (Jitter/Shimmer/HNR)\n",
    "        pp = call(snd, \"To PointProcess (periodic, cc)\", MIN_F0, MAX_F0)\n",
    "        jit_local = call(\n",
    "            pp, \"Get jitter (local)\", 0, 0,\n",
    "            JITTER_TIME_STEP, JITTER_MIN_PERIOD, JITTER_MAX_PERIOD\n",
    "        )\n",
    "        shim_local = call(\n",
    "            [snd, pp], \"Get shimmer (local)\", 0, 0,\n",
    "            SHIMMER_MIN_AMPLITUDE, SHIMMER_MAX_AMPLITUDE,\n",
    "            JITTER_MAX_PERIOD, SHIMMER_FACTOR\n",
    "        )\n",
    "        harmonicity = call(snd, \"To Harmonicity (cc)\", 0.01, 86.1329, 0.1, 1.0)\n",
    "        hnr_val = call(harmonicity, \"Get mean\", 0, 0)\n",
    "        if (hnr_val is None) or math.isnan(hnr_val):\n",
    "            hnr_val = 0.0\n",
    "\n",
    "        if hnr_val > 20.0:\n",
    "            hnr_cat = \"good\"\n",
    "        elif hnr_val > 15.0:\n",
    "            hnr_cat = \"acceptable\"\n",
    "        else:\n",
    "            hnr_cat = \"weak\"\n",
    "\n",
    "        # 2) Formants: pick midpoint of chunk\n",
    "        formant_obj = snd.to_formant_burg(\n",
    "            time_step=FORMANT_TIME_STEP,\n",
    "            max_number_of_formants=MAX_NUMBER_OF_FORMANTS,\n",
    "            maximum_formant=MAXIMUM_FORMANT_FREQUENCY\n",
    "        )\n",
    "        duration_s = snd.get_total_duration()\n",
    "        analysis_time = duration_s * 0.5\n",
    "        F1 = formant_obj.get_value_at_time(1, analysis_time) or 0.0\n",
    "        F2 = formant_obj.get_value_at_time(2, analysis_time) or 0.0\n",
    "        F3 = formant_obj.get_value_at_time(3, analysis_time) or 0.0\n",
    "\n",
    "        # 3) Vibrato from pitch contour\n",
    "        pitch_obj = snd.to_pitch_ac(\n",
    "            time_step=0.01,\n",
    "            voicing_threshold=0.6,\n",
    "            pitch_floor=MIN_F0,\n",
    "            pitch_ceiling=MAX_F0\n",
    "        )\n",
    "        pitch_values = pitch_obj.selected_array['frequency']\n",
    "        times = pitch_obj.xs()  # array of times for each pitch sample\n",
    "        # Filter out unvoiced or 0.0\n",
    "        valid_indices = np.where(pitch_values > 0.0)[0]\n",
    "        if len(valid_indices) < 4:\n",
    "            vib_extent = 0.0\n",
    "            vib_rate = 0.0\n",
    "        else:\n",
    "            pitched_times = times[valid_indices]\n",
    "            pitched_freqs = pitch_values[valid_indices]\n",
    "\n",
    "            # 3A) Convert to \"cents around median pitch\"\n",
    "            median_pitch = np.median(pitched_freqs)\n",
    "            if median_pitch <= 0.0:\n",
    "                median_pitch = 1e-6\n",
    "            pitch_cents = 1200.0 * np.log2(pitched_freqs / median_pitch)\n",
    "\n",
    "            # 3B) Vibrato extent: standard deviation of pitch_cents\n",
    "            vib_extent = float(np.std(pitch_cents))\n",
    "\n",
    "            # 3C) Vibrato rate: we find the main frequency in [3..10 Hz] via Welch\n",
    "            #     If chunk is short (< ~0.3s), we might not detect a strong vibrato peak.\n",
    "            fs_pitch = 1.0 / (float(pitch_obj.get_time_step()))  # pitch sampling rate\n",
    "            freqs_w, psd_w = welch(pitch_cents, fs=fs_pitch, nperseg=len(pitch_cents)//2 or 1)\n",
    "            # band-limit to [3..10 Hz]\n",
    "            vib_band = np.where((freqs_w >= VIBRATO_MIN_HZ) & (freqs_w <= VIBRATO_MAX_HZ))[0]\n",
    "            if len(vib_band) < 1:\n",
    "                vib_rate = 0.0\n",
    "            else:\n",
    "                freqs_sub = freqs_w[vib_band]\n",
    "                psd_sub = psd_w[vib_band]\n",
    "                peak_idx = np.argmax(psd_sub)\n",
    "                vib_rate = float(freqs_sub[peak_idx])  # Hz\n",
    "\n",
    "        out = {\n",
    "            \"praat_hnr\": float(hnr_val),\n",
    "            \"hnr_category\": hnr_cat,\n",
    "            \"jitter\": float(jit_local) if jit_local is not None else 0.0,\n",
    "            \"shimmer\": float(shim_local) if shim_local is not None else 0.0,\n",
    "            \"formants\": {\n",
    "                \"F1\": float(F1),\n",
    "                \"F2\": float(F2),\n",
    "                \"F3\": float(F3)\n",
    "            },\n",
    "            \"vibrato_extent\": vib_extent,\n",
    "            \"vibrato_rate\": vib_rate\n",
    "        }\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return out\n",
    "\n",
    "# ============== Build entire spectral_data =============\n",
    "# TODO: With the changes we're making to time_matrix_small, this will not get called\n",
    "# Delete it later\n",
    "def build_spectral_data(audio_data, sr, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH):\n",
    "    # If audio_data < n_fft, pad it to avoid warnings\n",
    "    if len(audio_data) < n_fft:\n",
    "        pad_len = n_fft - len(audio_data)\n",
    "        audio_data = np.pad(audio_data, (0, pad_len), 'constant')\n",
    "\n",
    "    S_complex = librosa.stft(audio_data, n_fft=n_fft, hop_length=hop_length)\n",
    "    S_mag = np.abs(S_complex)\n",
    "    S_log = librosa.amplitude_to_db(S_mag, ref=np.max)\n",
    "\n",
    "    power_spectrogram = S_mag**2\n",
    "    mfcc_data = librosa.feature.mfcc(\n",
    "        S=librosa.power_to_db(power_spectrogram),\n",
    "        sr=sr,\n",
    "        n_mfcc=N_MFCC\n",
    "    )\n",
    "    mfcc_delta = librosa.feature.delta(mfcc_data)\n",
    "\n",
    "    spectral_data = {\n",
    "        \"spectrogram_magnitude\": S_mag.tolist(),\n",
    "        \"spectrogram_log_db\": S_log.tolist(),\n",
    "        \"mfcc\": mfcc_data.tolist(),\n",
    "        \"delta_mfcc\": mfcc_delta.tolist()\n",
    "    }\n",
    "    return spectral_data\n",
    "\n",
    "\n",
    "# ============== Build time_matrix_small =============\n",
    "def build_time_matrix_small(\n",
    "    audio_data,\n",
    "    sr,\n",
    "    file_path,\n",
    "    base_pitch,\n",
    "    frame_size,\n",
    "    hop_length,\n",
    "    n_mfcc\n",
    "):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import librosa\n",
    "    import librosa.display\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pyloudnorm as pyln\n",
    "\n",
    "    # If audio is too short, zero-pad to avoid n_fft warnings\n",
    "    if len(audio_data) < frame_size:\n",
    "        pad_len = frame_size - len(audio_data)\n",
    "        audio_data = np.pad(audio_data, (0, pad_len), mode='constant')\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 1) Compute & Save the MEL-SPECTROGRAM as a PNG\n",
    "    # -------------------------------------------------\n",
    "    # Purely extracted for the AST model fine tuning\n",
    "    # This is a second load but being done solely for the AST model\n",
    "    mel_y, mel_sr = librosa.load(file_path, sr=16000)   # Force 16 kHz\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=mel_y,\n",
    "        sr=mel_sr,\n",
    "        n_fft=400,\n",
    "        hop_length=160,\n",
    "        n_mels=128\n",
    "    )\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Create a roughly 224×224 PNG in grayscale\n",
    "    fig = plt.figure(figsize=(3.11, 3.11), dpi=72)  # ~224×224 px\n",
    "    ax = fig.add_subplot(111)\n",
    "    img = librosa.display.specshow(\n",
    "        log_mel_spec,\n",
    "        sr=sr,\n",
    "        x_axis='time',\n",
    "        y_axis='mel',\n",
    "        cmap='gray',\n",
    "        ax=ax\n",
    "    )\n",
    "    plt.title('Mel Spectrogram')\n",
    "    plt.axis('off')  # no axes or ticks\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)  # ensure output dir\n",
    "    mel_filename = os.path.join(OUTPUT_DIR, f\"{base_name}_mel.png\")\n",
    "    plt.savefig(mel_filename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2) Now do your existing STFT-based analysis\n",
    "    # -------------------------------------------------\n",
    "    S_complex = librosa.stft(audio_data, n_fft=frame_size, hop_length=hop_length, center=False)\n",
    "    S_mag = np.abs(S_complex)\n",
    "    S_power = S_mag**2\n",
    "    S_log = librosa.amplitude_to_db(S_mag, ref=np.max)\n",
    "\n",
    "    # e.g., local ZCR, centroid, rolloff, bandwidth, RMS, LUFS, Pyin pitch, etc.\n",
    "    zcr_data = librosa.feature.zero_crossing_rate(audio_data, frame_length=frame_size, hop_length=hop_length)[0]\n",
    "    centroid = librosa.feature.spectral_centroid(S=S_mag)[0]\n",
    "    rolloff  = librosa.feature.spectral_rolloff(S=S_mag)[0]\n",
    "    bw       = librosa.feature.spectral_bandwidth(S=S_mag)[0]\n",
    "    rms_vals = librosa.feature.rms(y=audio_data, frame_length=frame_size, hop_length=hop_length)[0]\n",
    "\n",
    "    meter = pyln.Meter(sr)\n",
    "    min_required = int(0.4 * sr)\n",
    "\n",
    "    def compute_lufs_for_frame(i):\n",
    "        start_s = i * hop_length\n",
    "        end_s   = start_s + frame_size\n",
    "        if end_s > len(audio_data):\n",
    "            end_s = len(audio_data)\n",
    "        chunk = audio_data[start_s:end_s]\n",
    "        if len(chunk) >= min_required:\n",
    "            return meter.integrated_loudness(chunk)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    from librosa.feature import rhythm\n",
    "    def compute_local_tempo_for_frame(i):\n",
    "        start_s = i * hop_length\n",
    "        end_s = start_s + frame_size\n",
    "        if end_s > len(audio_data):\n",
    "            end_s = len(audio_data)\n",
    "        chunk = audio_data[start_s:end_s]\n",
    "        if len(chunk) < 32:\n",
    "            return 0.0\n",
    "        tmp = rhythm.tempo(y=chunk, sr=sr, hop_length=hop_length, aggregate=None)\n",
    "        return float(np.mean(tmp)) if tmp is not None and len(tmp) > 0 else 0.0\n",
    "\n",
    "    # Pyin pitch\n",
    "    pitches, voiced_flags, confidences = librosa.pyin(\n",
    "        y=audio_data,\n",
    "        fmin=MIN_F0,\n",
    "        fmax=MAX_F0,\n",
    "        sr=sr,\n",
    "        frame_length=frame_size,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    times = librosa.times_like(pitches, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # MFCC_data if you want to store or summarize MFCC as well\n",
    "    MFCC_data = librosa.feature.mfcc(\n",
    "        S=librosa.power_to_db(S_power),\n",
    "        sr=sr,\n",
    "        n_mfcc=N_MFCC\n",
    "    )\n",
    "    MFCC_data = MFCC_data.T  # shape (#frames, n_mfcc)\n",
    "\n",
    "    time_matrix_small = []\n",
    "    prev_pitch = 0.0\n",
    "    n_frames = len(pitches)\n",
    "    all_mfcc_frames = []\n",
    "\n",
    "    for i in range(n_frames):\n",
    "        t_s = float(times[i])\n",
    "        pitch_val = pitches[i] if (pitches[i] is not None and not np.isnan(pitches[i])) else 0.0\n",
    "        conf_val  = float(confidences[i]) if confidences[i] is not None else 0.0\n",
    "        vf        = bool(voiced_flags[i]) if voiced_flags[i] is not None else False\n",
    "\n",
    "        sruti_class, note_name, note_freq_hz, dev_cents, dev_flag = map_pitch_to_sruti(pitch_val, base_pitch)\n",
    "\n",
    "        tone_val = 0.0\n",
    "        if i < S_mag.shape[1]:\n",
    "            tone_val = compute_tone_to_noise(S_mag[:, i])\n",
    "\n",
    "        if i == 0:\n",
    "            trans_score = 1.0\n",
    "        else:\n",
    "            trans_score = compute_transition_score(prev_pitch, pitch_val)\n",
    "        prev_pitch = pitch_val\n",
    "\n",
    "        local_bpm = compute_local_tempo_for_frame(i)\n",
    "\n",
    "        start_samp = i * hop_length\n",
    "        end_samp = start_samp + frame_size\n",
    "        if end_samp > len(audio_data):\n",
    "            end_samp = len(audio_data)\n",
    "        chunk_data = audio_data[start_samp:end_samp]\n",
    "        praat_dict = compute_praat_metrics(chunk_data, sr)\n",
    "\n",
    "        zcr_val = float(zcr_data[i]) if i < len(zcr_data) else 0.0\n",
    "        sc_val  = float(centroid[i])  if i < len(centroid)  else 0.0\n",
    "        ro_val  = float(rolloff[i])   if i < len(rolloff)   else 0.0\n",
    "        bw_val  = float(bw[i])        if i < len(bw)        else 0.0\n",
    "\n",
    "        rms_db_val = -60.0\n",
    "        if i < len(rms_vals):\n",
    "            raw_rms = rms_vals[i]\n",
    "            if raw_rms <= 0.0:\n",
    "                rms_db_val = -60.0\n",
    "            else:\n",
    "                rms_db_val = float(20.0 * np.log10(raw_rms + 1e-12))\n",
    "\n",
    "        lufs_val = compute_lufs_for_frame(i)\n",
    "\n",
    "        # store frame-level MFCC if desired\n",
    "        if i < MFCC_data.shape[0]:\n",
    "            mfcc_vec = MFCC_data[i]\n",
    "        else:\n",
    "            mfcc_vec = np.zeros(n_mfcc)\n",
    "        all_mfcc_frames.append(mfcc_vec)\n",
    "\n",
    "        row = {\n",
    "            \"time_s\": t_s,\n",
    "            \"pitch_hz\": pitch_val,\n",
    "            \"note_freq_hz\": note_freq_hz,\n",
    "            \"voiced_flag\": vf,\n",
    "            \"confidence\": conf_val,\n",
    "            \"sruti_class\": sruti_class,\n",
    "            \"note_name\": note_name,\n",
    "            \"deviation_cents\": dev_cents,\n",
    "            \"dev_flag\": dev_flag,\n",
    "            \"pitch_accuracy_category\": \"good\" if dev_flag == 0 else \"out_of_tune\",\n",
    "\n",
    "            \"tone_to_noise\": tone_val,\n",
    "            \"transition_score\": trans_score,\n",
    "\n",
    "            \"praat_hnr\": praat_dict[\"praat_hnr\"],\n",
    "            \"hnr_category\": praat_dict[\"hnr_category\"],\n",
    "            \"jitter\": praat_dict[\"jitter\"],\n",
    "            \"shimmer\": praat_dict[\"shimmer\"],\n",
    "            \"formants\": praat_dict[\"formants\"],\n",
    "            \"vibrato_extent\": praat_dict[\"vibrato_extent\"],\n",
    "            \"vibrato_rate\": praat_dict[\"vibrato_rate\"],\n",
    "\n",
    "            \"zcr\": zcr_val,\n",
    "            \"spec_centroid\": sc_val,\n",
    "            \"spec_rolloff\": ro_val,\n",
    "            \"spec_bandwidth\": bw_val,\n",
    "\n",
    "            \"rms_db\": rms_db_val,\n",
    "            \"lufs\": float(lufs_val),\n",
    "            \"tempo_bpm\": local_bpm\n",
    "        }\n",
    "        time_matrix_small.append(row)\n",
    "\n",
    "    # Summarize MFCC if you like:\n",
    "    all_mfcc_frames = np.array(all_mfcc_frames)\n",
    "    if len(all_mfcc_frames) > 0:\n",
    "        mfcc_mean = np.mean(all_mfcc_frames, axis=0).tolist()\n",
    "        mfcc_std  = np.std(all_mfcc_frames, axis=0).tolist()\n",
    "    else:\n",
    "        mfcc_mean = [0.0]*n_mfcc\n",
    "        mfcc_std  = [0.0]*n_mfcc\n",
    "\n",
    "    mfcc_summary = {\n",
    "        \"mfcc_mean\": mfcc_mean,\n",
    "        \"mfcc_std\":  mfcc_std\n",
    "    }\n",
    "\n",
    "    return time_matrix_small, S_mag, S_power, S_log, mfcc_summary\n",
    "\n",
    "\n",
    "\n",
    "# ============== tempo-based matrix for bigger chunks =============\n",
    "def build_tempo_matrix(audio_data, sr, chunk_size, overlap=0.5):\n",
    "    \"\"\"\n",
    "    Build a time matrix with chunk-based tempo, RMS, and LUFS,\n",
    "    zero-padding if chunk < n_fft to avoid warnings.\n",
    "    \"\"\"\n",
    "    from librosa.feature import rhythm\n",
    "    hop = int(chunk_size * (1.0 - overlap))\n",
    "    n_samps = len(audio_data)\n",
    "    chunks = []\n",
    "    idx = 0\n",
    "    chunk_index = 0\n",
    "    meter = pyln.Meter(sr)\n",
    "\n",
    "    while idx < n_samps:\n",
    "        start_samp = idx\n",
    "        end_samp = idx + chunk_size\n",
    "        if end_samp > n_samps:\n",
    "            end_samp = n_samps\n",
    "        chunk_data = audio_data[start_samp:end_samp]\n",
    "        start_time_s = float(start_samp / sr)\n",
    "\n",
    "        if len(chunk_data) < 32:\n",
    "            tempo_bpm = 0.0\n",
    "        else:\n",
    "            tmp = rhythm.tempo(y=chunk_data, sr=sr, hop_length=HOP_LENGTH, aggregate=None)\n",
    "            tempo_bpm = float(np.mean(tmp)) if tmp is not None and len(tmp) > 0 else 0.0\n",
    "\n",
    "        # RMS\n",
    "        if len(chunk_data) > 0:\n",
    "            chunk_rms = np.sqrt(np.mean(chunk_data**2))\n",
    "            chunk_rms_db = float(20.0 * np.log10(chunk_rms + 1e-12))\n",
    "        else:\n",
    "            chunk_rms_db = -60.0\n",
    "\n",
    "        # LUFS\n",
    "        if len(chunk_data) >= int(0.4 * sr):\n",
    "            lufs_val = meter.integrated_loudness(chunk_data)\n",
    "        else:\n",
    "            lufs_val = 0.0\n",
    "\n",
    "        row = {\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"start_time_s\": start_time_s,\n",
    "            \"tempo_bpm\": tempo_bpm,\n",
    "            \"rms_db\": chunk_rms_db,\n",
    "            \"lufs\": float(lufs_val)\n",
    "        }\n",
    "        chunks.append(row)\n",
    "\n",
    "        chunk_index += 1\n",
    "        idx += hop\n",
    "        if idx >= n_samps:\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# ============== Advanced note features =============\n",
    "def build_advanced_note_features(audio_data, sr):\n",
    "    # Dummy example\n",
    "    return [\n",
    "        {\n",
    "            \"start_time_s\": 0.0,\n",
    "            \"end_time_s\": 2.4,\n",
    "            \"pitch_mean\": 233.8,\n",
    "            \"vibrato_extent\": 0.42,\n",
    "            \"vibrato_rate\": 5.6\n",
    "        },\n",
    "        {\n",
    "            \"start_time_s\": 2.4,\n",
    "            \"end_time_s\": 5.1,\n",
    "            \"pitch_mean\": 236.1,\n",
    "            \"vibrato_extent\": 0.50,\n",
    "            \"vibrato_rate\": 5.3\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# ============== Build tempo and advanced features - Med and Large =============\n",
    "def build_tempo_and_advanced_features(\n",
    "    audio_data,\n",
    "    sr,\n",
    "    time_matrix_small,\n",
    "    chunk_size=None,\n",
    "    hop_size=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds a chunk-based matrix for \"large\" segments (~0.5s each) by:\n",
    "      1) Doing integrated LUFS on each chunk (0.5s, so >=0.4s threshold).\n",
    "      2) Aggregating short-frame data from `time_matrix_small` that falls within\n",
    "         [start_time_s, end_time_s] to compute average tempo, pitch, jitter, shimmer, formants, etc.\n",
    "      3) Computing a real vibrato measure from the pitch contour in that chunk.\n",
    "         vibrato_extent => std. dev in cents; vibrato_rate => peaks/sec in that pitch contour.\n",
    "\n",
    "    Returns: a list of dicts, each describing one chunk:\n",
    "        [\n",
    "          {\n",
    "            \"chunk_index\": ...,\n",
    "            \"start_time_s\": ...,\n",
    "            \"end_time_s\": ...,\n",
    "            \"lufs\": ...,\n",
    "            \"avg_tempo_bpm\": ...,\n",
    "            \"avg_pitch_hz\": ...,\n",
    "            \"avg_jitter\": ...,\n",
    "            \"avg_shimmer\": ...,\n",
    "            \"avg_hnr\": ...,\n",
    "            \"avg_formants\": {\"F1\":..., \"F2\":..., \"F3\":...},\n",
    "            \"vibrato_extent\": ...,\n",
    "            \"vibrato_rate\": ...\n",
    "          },\n",
    "          ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    from psycopg2.extras import Json  # if needed, not strictly necessary here\n",
    "\n",
    "    # If not specified, read from your global constants\n",
    "    if chunk_size is None:\n",
    "        chunk_size = TEMPO_CHUNK_SIZE_LARGE  # e.g., 22050\n",
    "    if hop_size is None:\n",
    "        hop_size = VOCAL_FEATURE_CHUNK_HOP   # e.g., 4096\n",
    "\n",
    "    meter = pyln.Meter(sr)\n",
    "    results = []\n",
    "\n",
    "    n_samps = len(audio_data)\n",
    "    chunk_index = 0\n",
    "    idx = 0\n",
    "\n",
    "    while idx < n_samps:\n",
    "        start_samp = idx\n",
    "        end_samp = min(idx + chunk_size, n_samps)\n",
    "        start_time_s = float(start_samp / sr)\n",
    "        end_time_s   = float(end_samp / sr)\n",
    "        chunk_data   = audio_data[start_samp:end_samp]\n",
    "\n",
    "        # 1) Compute integrated LUFS if chunk >= 0.4s\n",
    "        chunk_duration_s = end_time_s - start_time_s\n",
    "        if chunk_duration_s >= 0.4:\n",
    "            lufs_val = meter.integrated_loudness(chunk_data)\n",
    "        else:\n",
    "            lufs_val = 0.0\n",
    "\n",
    "        # 2) Find frames in time_matrix_small that fall in [start_time_s, end_time_s]\n",
    "        sub_frames = [\n",
    "            row for row in time_matrix_small\n",
    "            if row[\"time_s\"] >= start_time_s and row[\"time_s\"] < end_time_s\n",
    "        ]\n",
    "        if not sub_frames:\n",
    "            # no frames in this chunk => store zeros & continue\n",
    "            results.append({\n",
    "                \"chunk_index\": chunk_index,\n",
    "                \"start_time_s\": start_time_s,\n",
    "                \"end_time_s\": end_time_s,\n",
    "                \"lufs\": float(lufs_val),\n",
    "                \"avg_tempo_bpm\": 0.0,\n",
    "                \"avg_pitch_hz\": 0.0,\n",
    "                \"avg_jitter\": 0.0,\n",
    "                \"avg_shimmer\": 0.0,\n",
    "                \"avg_hnr\": 0.0,\n",
    "                \"avg_formants\": {\"F1\":0.0, \"F2\":0.0, \"F3\":0.0},\n",
    "                \"vibrato_extent\": 0.0,\n",
    "                \"vibrato_rate\": 0.0\n",
    "            })\n",
    "            chunk_index += 1\n",
    "            idx += hop_size\n",
    "            continue\n",
    "\n",
    "        # 3) Aggregations from sub_frames (tempo, pitch, jitter, shimmer, hnr, formants, etc.)\n",
    "        tempo_vals   = [f[\"tempo_bpm\"] for f in sub_frames if f[\"tempo_bpm\"] > 0.0]\n",
    "        pitch_vals   = [f[\"pitch_hz\"]  for f in sub_frames if f[\"pitch_hz\"]  > 0.0]\n",
    "        jitter_vals  = [f[\"jitter\"]    for f in sub_frames if f[\"jitter\"]    > 0.0]\n",
    "        shimmer_vals = [f[\"shimmer\"]   for f in sub_frames if f[\"shimmer\"]   > 0.0]\n",
    "        hnr_vals     = [f[\"praat_hnr\"] for f in sub_frames if f[\"praat_hnr\"] > 0.0]\n",
    "\n",
    "        # average formants\n",
    "        f1_list = []\n",
    "        f2_list = []\n",
    "        f3_list = []\n",
    "        for f in sub_frames:\n",
    "            form_dict = f.get(\"formants\", {})\n",
    "            F1 = form_dict.get(\"F1\", 0.0)\n",
    "            F2 = form_dict.get(\"F2\", 0.0)\n",
    "            F3 = form_dict.get(\"F3\", 0.0)\n",
    "            if F1>0: f1_list.append(F1)\n",
    "            if F2>0: f2_list.append(F2)\n",
    "            if F3>0: f3_list.append(F3)\n",
    "\n",
    "        avg_tempo   = float(np.mean(tempo_vals))   if tempo_vals   else 0.0\n",
    "        avg_pitch   = float(np.mean(pitch_vals))   if pitch_vals   else 0.0\n",
    "        avg_jitter  = float(np.mean(jitter_vals))  if jitter_vals  else 0.0\n",
    "        avg_shimmer = float(np.mean(shimmer_vals)) if shimmer_vals else 0.0\n",
    "        avg_hnr     = float(np.mean(hnr_vals))     if hnr_vals     else 0.0\n",
    "        mean_f1     = float(np.mean(f1_list))      if f1_list      else 0.0\n",
    "        mean_f2     = float(np.mean(f2_list))      if f2_list      else 0.0\n",
    "        mean_f3     = float(np.mean(f3_list))      if f3_list      else 0.0\n",
    "\n",
    "        # 4) Vibrato (real approach):\n",
    "        #    - Convert pitch to \"cents\" relative to chunk's average pitch\n",
    "        #    - vibrato_extent = std of that contour\n",
    "        #    - vibrato_rate = #peak cycles per second\n",
    "        if len(pitch_vals) < 2 or avg_pitch < 1.0:\n",
    "            vib_extent = 0.0\n",
    "            vib_rate   = 0.0\n",
    "        else:\n",
    "            # Convert pitch to cents around chunk's average pitch\n",
    "            pitch_cents = []\n",
    "            for p in pitch_vals:\n",
    "                cents_diff = 1200.0 * math.log2(p / avg_pitch) if p>0.0 else 0.0\n",
    "                pitch_cents.append(cents_diff)\n",
    "            vib_extent = float(np.std(pitch_cents))\n",
    "\n",
    "            # quick approach for vib_rate: find peaks in pitch_cents around zero\n",
    "            # We'll center it at 0 by subtracting mean if you prefer\n",
    "            # but since we used chunk's avg pitch, pitch_cents are ~0 mean anyway.\n",
    "            # We'll find positive peaks & negative peaks => sum\n",
    "            # then convert to \"cycles / second\" by chunk duration. \n",
    "            arr = np.array(pitch_cents)\n",
    "            # find positive peaks\n",
    "            pos_peaks, _ = find_peaks(arr, height=2.0, distance=2)  # tuned params\n",
    "            # find negative peaks\n",
    "            neg_peaks, _ = find_peaks(-arr, height=2.0, distance=2)\n",
    "            # total # of peaks\n",
    "            total_peaks = len(pos_peaks) + len(neg_peaks)\n",
    "\n",
    "            # Each vibrato \"cycle\" has at least 1 positive + 1 negative peak,\n",
    "            # so # cycles ~ total_peaks / 2\n",
    "            chunk_dur_s = end_time_s - start_time_s\n",
    "            if chunk_dur_s > 0:\n",
    "                vib_rate = (total_peaks / 2.0) / chunk_dur_s\n",
    "            else:\n",
    "                vib_rate = 0.0\n",
    "\n",
    "        row = {\n",
    "            \"chunk_index\": chunk_index,\n",
    "            \"start_time_s\": start_time_s,\n",
    "            \"end_time_s\": end_time_s,\n",
    "            \"lufs\": float(lufs_val),\n",
    "            \"avg_tempo_bpm\": avg_tempo,\n",
    "            \"avg_pitch_hz\":  avg_pitch,\n",
    "            \"avg_jitter\":    avg_jitter,\n",
    "            \"avg_shimmer\":   avg_shimmer,\n",
    "            \"avg_hnr\":       avg_hnr,\n",
    "            \"avg_formants\": {\n",
    "                \"F1\": mean_f1,\n",
    "                \"F2\": mean_f2,\n",
    "                \"F3\": mean_f3\n",
    "            },\n",
    "            \"vibrato_extent\": vib_extent,\n",
    "            \"vibrato_rate\":   vib_rate\n",
    "        }\n",
    "        results.append(row)\n",
    "\n",
    "        chunk_index += 1\n",
    "        idx += hop_size\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============== Summaries from time_matrix_small =============\n",
    "def build_summary(time_matrix_small,time_matrix_tempo_large):\n",
    "    pitch_devs = [abs(row[\"deviation_cents\"]) for row in time_matrix_small]\n",
    "    if pitch_devs:\n",
    "        mean_dev = float(np.mean(pitch_devs))\n",
    "        std_dev = float(np.std(pitch_devs))\n",
    "    else:\n",
    "        mean_dev, std_dev = 0.0, 0.0\n",
    "\n",
    "    tone_vals = [row[\"tone_to_noise\"] for row in time_matrix_small]\n",
    "    if tone_vals:\n",
    "        tone_mean = float(np.mean(tone_vals))\n",
    "        tone_std  = float(np.std(tone_vals))\n",
    "    else:\n",
    "        tone_mean, tone_std = 0.0, 0.0\n",
    "\n",
    "    hnr_vals = [row[\"praat_hnr\"] for row in time_matrix_small if row[\"praat_hnr\"] > 0]\n",
    "    if hnr_vals:\n",
    "        hnr_mean = float(np.mean(hnr_vals))\n",
    "        hnr_std  = float(np.std(hnr_vals))\n",
    "    else:\n",
    "        hnr_mean, hnr_std = 0.0, 0.0\n",
    "\n",
    "    rms_vals = [row[\"rms_db\"] for row in time_matrix_small]\n",
    "    if rms_vals:\n",
    "        avg_rms = float(np.mean(rms_vals))\n",
    "        min_rms = float(np.min(rms_vals))\n",
    "        max_rms = float(np.max(rms_vals))\n",
    "        std_rms = float(np.std(rms_vals))\n",
    "    else:\n",
    "        avg_rms = min_rms = max_rms = std_rms = 0.0\n",
    "\n",
    "    lufs_vals = [row[\"lufs\"] for row in time_matrix_tempo_large]\n",
    "    if lufs_vals:\n",
    "        avg_lufs = float(np.mean(lufs_vals))\n",
    "        min_lufs = float(np.min(lufs_vals))\n",
    "        max_lufs = float(np.max(lufs_vals))\n",
    "        std_lufs = float(np.std(lufs_vals))\n",
    "    else:\n",
    "        avg_lufs = min_lufs = max_lufs = std_lufs = 0.0\n",
    "\n",
    "    summary = {\n",
    "        \"pitch_deviation\": {\n",
    "            \"mean\": mean_dev,\n",
    "            \"std\": std_dev,\n",
    "            \"deviation_threshold\": float(DEVIATION_THRESHOLD)\n",
    "        },\n",
    "        \"tone_to_noise_ratio\": {\n",
    "            \"mean\": tone_mean,\n",
    "            \"std\": tone_std\n",
    "        },\n",
    "        \"praat\": {\n",
    "            \"hnr_mean\": hnr_mean,\n",
    "            \"hnr_std\": hnr_std\n",
    "        },\n",
    "        \"dynamics\": {\n",
    "            \"rms_db\": {\n",
    "                \"mean\": float(avg_rms),\n",
    "                \"min\": float(min_rms),\n",
    "                \"max\": float(max_rms),\n",
    "                \"range\": float(max_rms - min_rms),\n",
    "                \"std\": float(std_rms),\n",
    "                \"dynamic_range\": float(max_rms - min_rms)\n",
    "            },\n",
    "            \"lufs\": {\n",
    "                \"mean\": float(avg_lufs),\n",
    "                \"min\": float(min_lufs),\n",
    "                \"max\": float(max_lufs),\n",
    "                \"range\": float(max_lufs - min_lufs),\n",
    "                \"std\": float(std_lufs),\n",
    "                \"dynamic_range\": float(max_lufs - min_lufs)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "# ============== Master analysis function =============\n",
    "def analyze_audio_file(file_path):\n",
    "    \"\"\"\n",
    "    1) Preprocess\n",
    "    2) Build spectral_data\n",
    "    3) time_matrix_small\n",
    "    4) Medium/large tempo\n",
    "    5) advanced note features\n",
    "    6) summary\n",
    "    7) final JSON\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    harmonic_audio, sr = preprocess_audio(file_path)\n",
    "\n",
    "\n",
    "    # 2) Attempt to detect drone\n",
    "    drone_pitch = detect_drone_pitch(harmonic_audio, sr)\n",
    "\n",
    "    if drone_pitch > 1.0:\n",
    "        # Found a strong drone\n",
    "        final_base_pitch = drone_pitch\n",
    "        #print(f\"Drone found ~ {final_base_pitch:.2f} Hz, using as base pitch.\")\n",
    "    else:\n",
    "        # fallback to pyin\n",
    "        pyin_pitch = detect_base_pitch_with_pyin(harmonic_audio, sr, fmin=MIN_F0, fmax=MAX_F0)\n",
    "        final_base_pitch = pyin_pitch\n",
    "        #print(f\"No dominant drone; fallback => base pitch ~ {final_base_pitch:.2f} Hz\")\n",
    "\n",
    "    # Spectral data (pad if short)\n",
    "    #spectral_data = build_spectral_data(harmonic_audio, sr, n_fft=FRAME_SIZE, hop_length=HOP_LENGTH)\n",
    "\n",
    "    # time_matrix_small (pad if short)\n",
    "    time_matrix_small, S_mag, S_power, S_log, mfcc_summary = build_time_matrix_small(\n",
    "    audio_data=harmonic_audio,\n",
    "    sr=sr,\n",
    "    file_path=file_path,\n",
    "    base_pitch=final_base_pitch,\n",
    "    frame_size=FRAME_SIZE,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_mfcc=N_MFCC\n",
    "    )\n",
    "\n",
    "    # tempo-based bigger chunks\n",
    "    #time_matrix_tempo_medium = build_tempo_matrix(harmonic_audio, sr, TEMPO_CHUNK_SIZE_MEDIUM, overlap=0.5)\n",
    "    #time_matrix_tempo_large  = build_tempo_matrix(harmonic_audio, sr, TEMPO_CHUNK_SIZE_LARGE, overlap=0.5)\n",
    "\n",
    "    # advanced note features\n",
    "    #advanced_note_features = build_advanced_note_features(harmonic_audio, sr)\n",
    "\n",
    "    time_matrix_tempo_large = build_tempo_and_advanced_features(\n",
    "        audio_data=harmonic_audio,\n",
    "        sr=sr,\n",
    "        time_matrix_small=time_matrix_small,\n",
    "        chunk_size=TEMPO_CHUNK_SIZE_LARGE,   # 22050\n",
    "        hop_size=VOCAL_FEATURE_CHUNK_HOP     # 4096\n",
    "    )\n",
    "\n",
    "\n",
    "    # summary\n",
    "    summary_data = build_summary(time_matrix_small, time_matrix_tempo_large)\n",
    "    summary_data[\"base_pitch\"] = float(final_base_pitch)\n",
    "    summary_data[\"spectral_summary\"] = mfcc_summary\n",
    "\n",
    "    analysis_dict = {\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"sample_rate\": sr,\n",
    "        \"summary\": summary_data,\n",
    "        \"time_matrices\": {\n",
    "            \"time_matrix_small\": time_matrix_small,\n",
    "            #\"time_matrix_tempo_medium\": time_matrix_tempo_medium,\n",
    "            \"time_matrix_tempo_large\": time_matrix_tempo_large\n",
    "        },\n",
    "        #\"spectral_data\": spectral_data,\n",
    "        #\"advanced_note_features\": advanced_note_features,\n",
    "        \"advanced_vocal_stats\": {\n",
    "            # placeholders or advanced stats\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return analysis_dict, harmonic_audio, sr\n",
    "\n",
    "\n",
    "# ============== Final: grade_single_file with DB Insert =============\n",
    "def grade_single_file(file_name):\n",
    "    \"\"\"\n",
    "    1) Runs analyze_audio_file\n",
    "    2) Replaces NaNs with None for JSON\n",
    "    3) Saves to DB if SAVE_TO_DB\n",
    "    4) Optionally playback or plot\n",
    "    \"\"\"\n",
    "    input_path = os.path.join(INPUT_DIR, file_name)\n",
    "    analysis_dict, harmonic_audio, sr = analyze_audio_file(input_path)\n",
    "\n",
    "    # replace NaN => None\n",
    "    sanitized_analysis_dict = _replace_nan_with_none(analysis_dict)\n",
    "\n",
    "    if SAVE_TO_DB:\n",
    "        db = QuantumMusicDB()\n",
    "        db.create_tables()\n",
    "        rec_id = db.insert_analysis(\n",
    "            file_name=file_name,\n",
    "            sample_rate=sr,\n",
    "            analysis_data=sanitized_analysis_dict\n",
    "        )\n",
    "        #print(f\"Inserted analysis record with ID: {rec_id}\")\n",
    "        db.close()\n",
    "\n",
    "    # Optionally: playback harmonic audio if in IPython\n",
    "    #try:\n",
    "    #    display(Audio(data=harmonic_audio, rate=sr))\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "    return sanitized_analysis_dict\n",
    "\n",
    "\n",
    "def process_file(file, training_dir, output_dir, grade_single_file):\n",
    "    \"\"\"\n",
    "    Helper function to process a single file: runs grade_single_file and then moves the file.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(training_dir, file)\n",
    "    print(f\"Processing {file}...\")\n",
    "    try:\n",
    "        # This call is assumed to do the DB insertion internally:\n",
    "        result = grade_single_file(file)\n",
    "        # Move the file to output_dir after processing\n",
    "        shutil.move(file_path, os.path.join(output_dir, file))\n",
    "        print(f\"Processed & moved {file} -> {output_dir}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_all_files(\n",
    "    grade_single_file,\n",
    "    training_dir=\"data/trainingdata\",\n",
    "    output_dir=\"data/trainingdataoutput\",\n",
    "    num_workers=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes all .wav files in `training_dir` using the provided\n",
    "    `grade_single_file` function (which already inserts its results into the database),\n",
    "    then moves the processed files to `output_dir` using parallel execution.\n",
    "    \"\"\"\n",
    "    if not callable(grade_single_file):\n",
    "        raise ValueError(\"grade_single_file must be a callable that accepts a filename.\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # List only .wav files\n",
    "    files = [f for f in os.listdir(training_dir) if f.endswith(\".wav\")]\n",
    "    if not files:\n",
    "        print(f\"No  .wav files found in {training_dir}\")\n",
    "        return\n",
    "\n",
    "    # Use ProcessPoolExecutor to process files in parallel\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Submit each file as a separate job\n",
    "        futures = {\n",
    "            executor.submit(process_file, file, training_dir, output_dir, grade_single_file): file\n",
    "            for file in files\n",
    "        }\n",
    "        # Wait for each future to complete and handle exceptions if any\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            file = futures[future]\n",
    "            try:\n",
    "                _ = future.result()\n",
    "            except Exception as exc:\n",
    "                print(f\"Error processing {file}: {exc}\")\n",
    "\n",
    "    print(\"All .wav files processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", choices=[\"single\", \"all\"], default=\"single\")\n",
    "    parser.add_argument(\"--file\", type=str, default=\"Bhairav3Rohan.wav\")\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=\"data/trainingdata\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"data/trainingdataoutput\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.mode == \"single\":\n",
    "        process_file(\n",
    "            file=args.file,\n",
    "            training_dir=args.training_dir,\n",
    "            output_dir=args.output_dir,\n",
    "            grade_single_file=grade_single_file\n",
    "        )\n",
    "    else:\n",
    "        process_all_files(\n",
    "            grade_single_file=grade_single_file,\n",
    "            training_dir=args.training_dir,\n",
    "            output_dir=args.output_dir,\n",
    "            num_workers=args.num_workers\n",
    "        )\n",
    "    print(\"Analysis complete.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
