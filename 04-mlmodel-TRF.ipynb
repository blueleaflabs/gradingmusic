{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import ASTModel, ASTConfig\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "###############################################################################\n",
    "# MPS DEVICE CHECK\n",
    "###############################################################################\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using MPS device for GPU acceleration on Apple Silicon.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Falling back to CPU.\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# HYPERPARAMETERS / GLOBAL SETTINGS\n",
    "###############################################################################\n",
    "HYPERPARAMS = {\n",
    "    \"OUTPUT_DIR\": \"data/trainingdataoutput\",\n",
    "    \"DB_LIMIT\": 100000,               # We'll read up to this many records (if needed)\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"EPOCHS\": 10,                   # up to 10 or 20 epochs\n",
    "    \"LEARNING_RATE_MAIN\": 5e-5,     # for AST layers (if we unfreeze them)\n",
    "    \"LEARNING_RATE_HEAD\": 1e-4,     # for scalar/quantum/final layers\n",
    "    \"WEIGHT_DECAY\": 1e-4,           # L2 regularization\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"NUM_AST_LAYERS_UNFROZEN\": 4,   # partial unfreezing\n",
    "    \"PATIENCE\": 5,                  # early stopping patience\n",
    "    \"STOP_LIMIT\": 5,                # must train at least this many epochs\n",
    "\n",
    "    # AST model input shape for audio: (freq=128, time=1024) by default\n",
    "    \"AST_FREQ\": 128,\n",
    "    \"AST_TIME\": 1024\n",
    "}\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# DATABASE FETCH LOGIC (FETCH ONLY)\n",
    "###############################################################################\n",
    "class QuantumMusicDBFetchOnly:\n",
    "    \"\"\"\n",
    "    A minimal database utility class which fetches data from a Postgres table\n",
    "    named 'audio_analysis'. This specialized version only supports reading.\n",
    "    \"\"\"\n",
    "    def __init__(self, db_name=\"quantummusic_csef\", host=\"localhost\", user=\"postgres\", password=\"postgres\"):\n",
    "        import psycopg2\n",
    "        self.psycopg2 = psycopg2\n",
    "        self.db_name = db_name\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.conn = None\n",
    "        self.connect()\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            import psycopg2\n",
    "            self.conn = psycopg2.connect(\n",
    "                dbname=self.db_name,\n",
    "                host=self.host,\n",
    "                user=self.user,\n",
    "                password=self.password\n",
    "            )\n",
    "            print(f\"Connected to database {self.db_name}. (fetch-only)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to database: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed.\")\n",
    "\n",
    "    def fetch_all_thaat_records(self):\n",
    "        \"\"\"\n",
    "        Fetches all rows from 'audio_analysis' where file_name contains 'thaat'.\n",
    "        Returns list of (id, analysis_data).\n",
    "        \"\"\"\n",
    "        with self.conn.cursor() as cur:\n",
    "            query = \"\"\"\n",
    "                SELECT id, analysis_data\n",
    "                FROM audio_analysis\n",
    "                WHERE file_name ILIKE '%%thaat%%' limit 1000\n",
    "            \"\"\"\n",
    "            cur.execute(query)\n",
    "            rows = cur.fetchall()\n",
    "        return rows\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# HELPER FUNCTIONS\n",
    "###############################################################################\n",
    "def convert_counts_to_probs_feature(counts_dict, max_bits=10):\n",
    "    \"\"\"\n",
    "    Convert a dictionary of quantum measurement counts into a probability\n",
    "    distribution vector of length 2^max_bits.\n",
    "    \"\"\"\n",
    "    total_counts = sum(counts_dict.values())\n",
    "    if total_counts == 0:\n",
    "        return np.zeros(2**max_bits, dtype=np.float32)\n",
    "\n",
    "    feature_vec = np.zeros(2**max_bits, dtype=np.float32)\n",
    "    for bitstring, c in counts_dict.items():\n",
    "        if len(bitstring) > max_bits:\n",
    "            truncated = bitstring[-max_bits:]\n",
    "        else:\n",
    "            truncated = bitstring.rjust(max_bits, '0')\n",
    "        idx = int(truncated, 2)\n",
    "        feature_vec[idx] += c / total_counts\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def load_image_as_tensor(image_path: str):\n",
    "    \"\"\"\n",
    "    Loads an image as a 1x128x128 tensor (grayscale). Returns None if not found.\n",
    "    \"\"\"\n",
    "    transform_img = T.Compose([\n",
    "        T.Resize((128, 128)),\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    with Image.open(image_path) as img:\n",
    "        img = img.convert(\"L\")\n",
    "        return transform_img(img)  # => shape [1,128,128]\n",
    "\n",
    "\n",
    "def parse_thaat_and_raga(file_name: str):\n",
    "    \"\"\"\n",
    "    Parse from a file_name that *contains* 'thaat'.\n",
    "    Example: 'Kalyan_thaat_Bhoopali_xyz.wav'\n",
    "\n",
    "    - 'thaat' = everything before '_thaat'\n",
    "      => 'Kalyan'\n",
    "    - 'raga'  = the substring after 'thaat_' up to the next underscore\n",
    "      => 'Bhoopali'\n",
    "\n",
    "    Returns (thaat, raga). If something fails, returns (None, None).\n",
    "    \"\"\"\n",
    "    base = file_name.replace(\".wav\", \"\")\n",
    "    # We expect something like: 'Kalyan_thaat_Bhoopali_morestuff'\n",
    "    if \"thaat_\" not in base:\n",
    "        return None, None\n",
    "\n",
    "    parts = base.split(\"thaat_\")\n",
    "    if len(parts) != 2:\n",
    "        return None, None\n",
    "\n",
    "    # left_part => 'Kalyan'\n",
    "    thaat_str = parts[0].rstrip(\"_\")\n",
    "    # right_part => 'Bhoopali_morestuff'\n",
    "    right_part = parts[1]\n",
    "\n",
    "    # The raga is up to the next underscore in right_part\n",
    "    # e.g. 'Bhoopali_something' => 'Bhoopali'\n",
    "    raga_parts = right_part.split(\"_\", 1)\n",
    "    raga_str = raga_parts[0]\n",
    "\n",
    "    #print(f\"Parsed thaat: {thaat_str}, raga: {raga_str} from file_name: {file_name}\")\n",
    "\n",
    "    return thaat_str, raga_str\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# MAIN DATA FETCH + PARSE\n",
    "###############################################################################\n",
    "def fetch_training_data_thaat_raga(limit=None):\n",
    "    \"\"\"\n",
    "    1) Fetch all DB records whose file_name contains 'thaat'.\n",
    "    2) Parse (thaat, raga) from file_name.\n",
    "    3) Use ALL such records, with no exclusions.\n",
    "    4) Build our data arrays:\n",
    "        - mel spectrogram image\n",
    "        - scalar features\n",
    "        - quantum features\n",
    "        - integer-encoded thaat, raga\n",
    "    5) Return (data_dict, used_ids).\n",
    "\n",
    "    The structure of `analysis_data` is expected to have:\n",
    "        {\n",
    "          \"file_name\": \"...\",\n",
    "          \"summary\": { ... },\n",
    "          \"time_matrices\": { ... },\n",
    "          \"quantum_analysis\": { ... }\n",
    "        }\n",
    "    \"\"\"\n",
    "    db = QuantumMusicDBFetchOnly()\n",
    "    rows = db.fetch_all_thaat_records()\n",
    "    db.close()\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No 'thaat' data found in DB.\")\n",
    "        return None, []\n",
    "\n",
    "    included_records = []\n",
    "    for (record_id, analysis_data) in rows:\n",
    "        fname = analysis_data[\"file_name\"]\n",
    "        thaat, raga = parse_thaat_and_raga(fname)\n",
    "        if thaat is not None and raga is not None:\n",
    "            included_records.append((record_id, analysis_data))\n",
    "\n",
    "    # If needed, we can limit how many total records we keep\n",
    "    if limit is not None and len(included_records) > limit:\n",
    "        included_records = included_records[:limit]\n",
    "\n",
    "    if not included_records:\n",
    "        print(\"No data left after filtering.\")\n",
    "        return None, []\n",
    "\n",
    "    # Prepare arrays\n",
    "    images_mel = []\n",
    "    scalar_feats = []\n",
    "    quantum_feats = []\n",
    "    thaat_labels = []\n",
    "    raga_labels = []\n",
    "    used_ids = []\n",
    "\n",
    "    # We'll collect distinct thaats, ragas\n",
    "    all_thaats = set()\n",
    "    all_ragas = set()\n",
    "\n",
    "    print(f\"Total included records: {len(included_records)}\")\n",
    "\n",
    "    for (record_id, analysis_data) in included_records:\n",
    "        fname = analysis_data[\"file_name\"]\n",
    "        thaat, raga = parse_thaat_and_raga(fname)\n",
    "        if thaat is None or raga is None:\n",
    "            continue\n",
    "\n",
    "        used_ids.append(record_id)\n",
    "        all_thaats.add(thaat)\n",
    "        all_ragas.add(raga)\n",
    "\n",
    "        # Load mel-spectrogram\n",
    "        base_no_ext = fname.replace(\".wav\", \"\")\n",
    "        mel_path = os.path.join(\"data\", \"analysisoutput\", f\"{base_no_ext}_mel.png\")\n",
    "        mel_img = load_image_as_tensor(mel_path)\n",
    "        if mel_img is None:\n",
    "            # fallback\n",
    "            mel_img = torch.zeros((1, 128, 128))\n",
    "\n",
    "        images_mel.append(mel_img)\n",
    "\n",
    "        # Extract scalar features\n",
    "        summary_dict = analysis_data.get(\"summary\", {})\n",
    "        pitch_dev = summary_dict.get(\"pitch_deviation\", {})\n",
    "        tnr_dict  = summary_dict.get(\"tone_to_noise_ratio\", {})\n",
    "        praat_dict= summary_dict.get(\"praat\", {})\n",
    "        dynamics  = summary_dict.get(\"dynamics\", {})\n",
    "\n",
    "        avg_dev = pitch_dev.get(\"mean\", 0.0)\n",
    "        std_dev = pitch_dev.get(\"std\", 0.0)\n",
    "        avg_tnr = tnr_dict.get(\"mean\", 0.0)\n",
    "        avg_hnr = praat_dict.get(\"hnr_mean\", 0.0)\n",
    "\n",
    "        rms_db_stats = dynamics.get(\"rms_db\", {})\n",
    "        mean_rms_db  = rms_db_stats.get(\"mean\", 0.0)\n",
    "        lufs_stats   = dynamics.get(\"lufs\", {})\n",
    "        mean_lufs    = lufs_stats.get(\"mean\", 0.0)\n",
    "\n",
    "        time_matrices = analysis_data.get(\"time_matrices\", {})\n",
    "        time_matrix_small = time_matrices.get(\"time_matrix_small\", [])\n",
    "\n",
    "        jitter_vals = [x.get(\"jitter\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "        shimmer_vals= [x.get(\"shimmer\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "        vib_vals    = [x.get(\"vibrato_rate\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "        f1_vals     = [x.get(\"formants\", {}).get(\"F1\", 0.0) or 0.0 for x in time_matrix_small]\n",
    "\n",
    "        avg_jitter  = float(np.mean(jitter_vals))  if jitter_vals else 0.0\n",
    "        avg_shimmer = float(np.mean(shimmer_vals)) if shimmer_vals else 0.0\n",
    "        avg_vibrato = float(np.mean(vib_vals))      if vib_vals else 0.0\n",
    "        avg_formant = float(np.mean(f1_vals))      if f1_vals else 0.0\n",
    "\n",
    "        scalars = [\n",
    "            avg_dev, std_dev,\n",
    "            avg_hnr, avg_tnr,\n",
    "            mean_rms_db, mean_lufs,\n",
    "            avg_jitter, avg_shimmer,\n",
    "            avg_vibrato, avg_formant\n",
    "        ]\n",
    "        scalar_feats.append(scalars)\n",
    "\n",
    "        # Quantum features\n",
    "        quantum_dict = analysis_data.get(\"quantum_analysis\", {})\n",
    "        angles = quantum_dict.get(\"scaled_angles\", [])\n",
    "        max_len = 10\n",
    "        angle_arr = np.zeros(max_len, dtype=np.float32)\n",
    "        for i in range(min(max_len, len(angles))):\n",
    "            angle_arr[i] = angles[i]\n",
    "\n",
    "        counts_d = quantum_dict.get(\"measurement_counts\", {})\n",
    "        dist_vec = convert_counts_to_probs_feature(counts_d, max_bits=10)\n",
    "        combined_q = np.concatenate([angle_arr, dist_vec], axis=0)\n",
    "        quantum_feats.append(combined_q)\n",
    "\n",
    "        # We'll stash the raw thaat/raga strings for mapping\n",
    "        thaat_labels.append(thaat)\n",
    "        raga_labels.append(raga)\n",
    "\n",
    "    # Build label maps\n",
    "    unique_thaats = sorted(list(all_thaats))\n",
    "    unique_ragas  = sorted(list(all_ragas))\n",
    "    thaat_to_idx  = {t: i for i, t in enumerate(unique_thaats)}\n",
    "    raga_to_idx   = {r: i for i, r in enumerate(unique_ragas)}\n",
    "\n",
    "    # Encode\n",
    "    label_thaat_idx = [thaat_to_idx[t] for t in thaat_labels]\n",
    "    label_raga_idx  = [raga_to_idx[r]  for r in raga_labels]\n",
    "\n",
    "    # Final data structures\n",
    "    images_mel    = torch.stack(images_mel)  # shape => (N, 1, 128, 128)\n",
    "    scalar_feats  = np.array(scalar_feats, dtype=np.float32)\n",
    "    quantum_feats = np.array(quantum_feats, dtype=np.float32)\n",
    "\n",
    "    data_dict = {\n",
    "        \"images_mel\": images_mel,\n",
    "        \"scalar_feats\": scalar_feats,\n",
    "        \"quantum_feats\": quantum_feats,\n",
    "        \"label_thaat_idx\": np.array(label_thaat_idx, dtype=np.int64),\n",
    "        \"label_raga_idx\": np.array(label_raga_idx, dtype=np.int64),\n",
    "        \"thaat_to_idx\": thaat_to_idx,\n",
    "        \"raga_to_idx\": raga_to_idx,\n",
    "        \"unique_thaats\": unique_thaats,\n",
    "        \"unique_ragas\": unique_ragas\n",
    "    }\n",
    "\n",
    "    return data_dict, used_ids\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# DATASET & MODELS\n",
    "###############################################################################\n",
    "class MultiLabelThaatRagaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each sample has:\n",
    "       1) mel_imgs\n",
    "       2) scalar features\n",
    "       3) quantum features\n",
    "       4) thaat label\n",
    "       5) raga label\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.mel_imgs   = data_dict[\"images_mel\"]\n",
    "        self.scalars    = data_dict[\"scalar_feats\"]\n",
    "        self.quants     = data_dict[\"quantum_feats\"]\n",
    "        self.thaat_lbl  = data_dict[\"label_thaat_idx\"]\n",
    "        self.raga_lbl   = data_dict[\"label_raga_idx\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.thaat_lbl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scal  = torch.tensor(self.scalars[idx], dtype=torch.float32)\n",
    "        quan  = torch.tensor(self.quants[idx], dtype=torch.float32)\n",
    "        t_lbl = torch.tensor(self.thaat_lbl[idx], dtype=torch.long)\n",
    "        r_lbl = torch.tensor(self.raga_lbl[idx], dtype=torch.long)\n",
    "        return (\n",
    "            self.mel_imgs[idx],\n",
    "            scal,\n",
    "            quan,\n",
    "            t_lbl,\n",
    "            r_lbl\n",
    "        )\n",
    "\n",
    "\n",
    "class HybridASTModelThaatRaga(nn.Module):\n",
    "    \"\"\"\n",
    "    Similar to a \"hybrid\" AST model, but predicting:\n",
    "      1) Thaat\n",
    "      2) Raga\n",
    "    (no quality label).\n",
    "\n",
    "    The AST portion is partially unfrozen. We then fuse scalar + quantum MLPs\n",
    "    with the AST embedding. Finally, we have two heads: (thaat_head, raga_head).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_thaats, num_ragas, scalar_dim=10, quantum_dim=(10 + 2**10), num_unfrozen_layers=0):\n",
    "        super().__init__()\n",
    "        # Load AST model\n",
    "        self.config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config=self.config)\n",
    "\n",
    "        # Freeze the AST\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last N encoder layers\n",
    "        if num_unfrozen_layers > 0:\n",
    "            total_layers = 12\n",
    "            start_layer = max(0, total_layers - num_unfrozen_layers)\n",
    "            for layer_idx in range(start_layer, total_layers):\n",
    "                for param in self.ast_model.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # MLP for scalar features\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # MLP for quantum features\n",
    "        self.quantum_fc = nn.Sequential(\n",
    "            nn.Linear(quantum_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        # The AST base output is 768 dims\n",
    "        combined_dim = 768 + 64 + 64\n",
    "\n",
    "        # HEADS\n",
    "        self.thaat_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_thaats)\n",
    "        )\n",
    "        self.raga_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_ragas)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_img, scal_in, quan_in):\n",
    "        # mel_img => (B,1,H,W)\n",
    "        B, C, H, W = mel_img.shape\n",
    "        freq = HYPERPARAMS[\"AST_FREQ\"]\n",
    "        time = HYPERPARAMS[\"AST_TIME\"]\n",
    "\n",
    "        # Resize to (B,1,128,1024)\n",
    "        mel_resized = F.interpolate(\n",
    "            mel_img, size=(freq, time),\n",
    "            mode='bilinear', align_corners=False\n",
    "        )\n",
    "        # AST wants shape (B, freq, time) => remove channel dim\n",
    "        mel_for_ast = mel_resized.squeeze(1)\n",
    "\n",
    "        outputs = self.ast_model(input_values=mel_for_ast, output_hidden_states=True)\n",
    "        hidden = outputs.last_hidden_state  # (B, seq_len, 768)\n",
    "        ast_embedding = hidden[:, 0, :]     # [CLS] => (B,768)\n",
    "\n",
    "        emb_scal = self.scalar_fc(scal_in)    # (B,64)\n",
    "        emb_quan = self.quantum_fc(quan_in)   # (B,64)\n",
    "\n",
    "        fused = torch.cat([ast_embedding, emb_scal, emb_quan], dim=1)  # (B, 768+64+64=896)\n",
    "\n",
    "        logits_thaat = self.thaat_head(fused)  # (B, num_thaats)\n",
    "        logits_raga  = self.raga_head(fused)   # (B, num_ragas)\n",
    "        return logits_thaat, logits_raga\n",
    "\n",
    "\n",
    "class HybridASTModelThaatRagaNoQuantum(nn.Module):\n",
    "    \"\"\"\n",
    "    A variant of HybridASTModelThaatRaga with NO quantum features fused in.\n",
    "    We only combine AST + scalar features => 768 + 64 => 832 dims,\n",
    "    then have two classification heads (thaat, raga).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_thaats, num_ragas, scalar_dim=10, num_unfrozen_layers=0):\n",
    "        super().__init__()\n",
    "        self.config = ASTConfig.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.ast_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", config=self.config)\n",
    "\n",
    "        # Freeze the AST\n",
    "        for param in self.ast_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the last N encoder layers if requested\n",
    "        if num_unfrozen_layers > 0:\n",
    "            total_layers = 12\n",
    "            start_layer = max(0, total_layers - num_unfrozen_layers)\n",
    "            for layer_idx in range(start_layer, total_layers):\n",
    "                for param in self.ast_model.encoder.layer[layer_idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Only scalar MLP\n",
    "        self.scalar_fc = nn.Sequential(\n",
    "            nn.Linear(scalar_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "\n",
    "        combined_dim = 768 + 64\n",
    "\n",
    "        self.thaat_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_thaats)\n",
    "        )\n",
    "        self.raga_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_ragas)\n",
    "        )\n",
    "\n",
    "    def forward(self, mel_img, scalar_input):\n",
    "        B, C, H, W = mel_img.shape\n",
    "        freq = HYPERPARAMS[\"AST_FREQ\"]\n",
    "        time = HYPERPARAMS[\"AST_TIME\"]\n",
    "\n",
    "        mel_resized = F.interpolate(\n",
    "            mel_img, size=(freq, time),\n",
    "            mode='bilinear', align_corners=False\n",
    "        )\n",
    "        mel_for_ast = mel_resized.squeeze(1)\n",
    "\n",
    "        outputs = self.ast_model(input_values=mel_for_ast, output_hidden_states=True)\n",
    "        hidden = outputs.last_hidden_state  # (B, seq_len, 768)\n",
    "        ast_embedding = hidden[:, 0, :]     # (B,768)\n",
    "\n",
    "        emb_scal = self.scalar_fc(scalar_input)  # (B,64)\n",
    "        fused = torch.cat([ast_embedding, emb_scal], dim=1)  # (B,832)\n",
    "\n",
    "        logits_thaat = self.thaat_head(fused)\n",
    "        logits_raga  = self.raga_head(fused)\n",
    "        return logits_thaat, logits_raga\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# TRAINING FUNCTIONS\n",
    "###############################################################################\n",
    "def train_thaat_raga_model(include_quantum=True):\n",
    "    \"\"\"\n",
    "    Main training function for the new Thaat+Raga classification.\n",
    "\n",
    "    If `include_quantum=True`, we use HybridASTModelThaatRaga,\n",
    "    otherwise we use HybridASTModelThaatRagaNoQuantum.\n",
    "\n",
    "    Returns (model, data_dict).\n",
    "    \"\"\"\n",
    "    # Fetch data\n",
    "    data_dict, used_ids = fetch_training_data_thaat_raga(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"No data to train.\")\n",
    "        return None, None\n",
    "\n",
    "    # Create dataset\n",
    "    dataset_full = MultiLabelThaatRagaDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "    train_dl = DataLoader(train_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=True)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    scalar_feats = data_dict[\"scalar_feats\"]\n",
    "    quantum_feats= data_dict[\"quantum_feats\"]\n",
    "    num_thaats   = len(data_dict[\"thaat_to_idx\"])\n",
    "    num_ragas    = len(data_dict[\"raga_to_idx\"])\n",
    "\n",
    "    # Build model\n",
    "    if include_quantum:\n",
    "        model = HybridASTModelThaatRaga(\n",
    "            num_thaats=num_thaats,\n",
    "            num_ragas=num_ragas,\n",
    "            scalar_dim=scalar_feats.shape[1],\n",
    "            quantum_dim=quantum_feats.shape[1],\n",
    "            num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "        ).to(HYPERPARAMS[\"DEVICE\"])\n",
    "    else:\n",
    "        model = HybridASTModelThaatRagaNoQuantum(\n",
    "            num_thaats=num_thaats,\n",
    "            num_ragas=num_ragas,\n",
    "            scalar_dim=scalar_feats.shape[1],\n",
    "            num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "        ).to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=HYPERPARAMS[\"LEARNING_RATE_HEAD\"],\n",
    "        weight_decay=HYPERPARAMS[\"WEIGHT_DECAY\"]\n",
    "    )\n",
    "    crit_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    EPOCHS = HYPERPARAMS[\"EPOCHS\"]\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    patience = HYPERPARAMS[\"PATIENCE\"]\n",
    "    epochs_no_improve = 0\n",
    "    stop_limit = HYPERPARAMS[\"STOP_LIMIT\"]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for batch in train_dl:\n",
    "            if include_quantum:\n",
    "                mel_img, scal, quan, t_lbl, r_lbl = batch\n",
    "                mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                t_lbl   = t_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                r_lbl   = r_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits_thaat, logits_raga = model(mel_img, scal, quan)\n",
    "            else:\n",
    "                mel_img, scal, quan, t_lbl, r_lbl = batch\n",
    "                mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                t_lbl   = t_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                r_lbl   = r_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits_thaat, logits_raga = model(mel_img, scal)\n",
    "\n",
    "            loss_thaat = crit_ce(logits_thaat, t_lbl)\n",
    "            loss_raga  = crit_ce(logits_raga, r_lbl)\n",
    "            loss = loss_thaat + loss_raga\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dl)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # EVAL\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dl:\n",
    "                if include_quantum:\n",
    "                    mel_img, scal, quan, t_lbl, r_lbl = batch\n",
    "                    mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                    scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                    quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                    t_lbl   = t_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                    r_lbl   = r_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                    logits_thaat, logits_raga = model(mel_img, scal, quan)\n",
    "                else:\n",
    "                    mel_img, scal, quan, t_lbl, r_lbl = batch\n",
    "                    mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                    scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                    t_lbl   = t_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                    r_lbl   = r_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                    logits_thaat, logits_raga = model(mel_img, scal)\n",
    "\n",
    "                loss_thaat = crit_ce(logits_thaat, t_lbl)\n",
    "                loss_raga  = crit_ce(logits_raga, r_lbl)\n",
    "                total_test_loss += (loss_thaat + loss_raga).item()\n",
    "\n",
    "        avg_test_loss = total_test_loss / len(test_dl)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        prefix = \"[Quantum]\" if include_quantum else \"[NoQuantum]\"\n",
    "        print(f\"{prefix} Epoch {epoch+1}/{EPOCHS}: train_loss={avg_train_loss:.4f}, test_loss={avg_test_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_test_loss < best_test_loss:\n",
    "            best_test_loss = avg_test_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if (epoch + 1) >= stop_limit and epochs_no_improve >= patience:\n",
    "                print(f\"{prefix} No improvement for {patience} epochs after epoch {stop_limit}. Stopping.\")\n",
    "                break\n",
    "\n",
    "    print(f\"{prefix} Training complete or early stopped.\")\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Final confusion matrices + accuracies\n",
    "    all_thaat_preds, all_thaat_truth = [], []\n",
    "    all_raga_preds, all_raga_truth   = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dl:\n",
    "            if include_quantum:\n",
    "                mel_img, scal, quan, t_lbl, r_lbl = batch\n",
    "                mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                t_lbl   = t_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                r_lbl   = r_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                logits_thaat, logits_raga = model(mel_img, scal, quan)\n",
    "            else:\n",
    "                mel_img, scal, quan, t_lbl, r_lbl = batch\n",
    "                mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                t_lbl   = t_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "                r_lbl   = r_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "                logits_thaat, logits_raga = model(mel_img, scal)\n",
    "\n",
    "            pred_thaat = logits_thaat.argmax(dim=1).cpu().numpy()\n",
    "            pred_raga  = logits_raga.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_thaat_preds.extend(pred_thaat)\n",
    "            all_thaat_truth.extend(t_lbl.cpu().numpy())\n",
    "            all_raga_preds.extend(pred_raga)\n",
    "            all_raga_truth.extend(r_lbl.cpu().numpy())\n",
    "\n",
    "    # Compute confusion matrix for thaat\n",
    "    cm_thaat = confusion_matrix(all_thaat_truth, all_thaat_preds)\n",
    "    used_thaat_indices = sorted(set(all_thaat_truth) | set(all_thaat_preds))\n",
    "    idx_to_thaat = {v: k for k, v in data_dict[\"thaat_to_idx\"].items()}\n",
    "    used_thaat_labels = [idx_to_thaat[i] for i in used_thaat_indices]\n",
    "\n",
    "    disp_thaat = ConfusionMatrixDisplay(cm_thaat, display_labels=used_thaat_labels)\n",
    "    disp_thaat.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Thaat Confusion Matrix {prefix}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Compute confusion matrix for raga\n",
    "    cm_raga = confusion_matrix(all_raga_truth, all_raga_preds)\n",
    "    used_raga_indices = sorted(set(all_raga_truth) | set(all_raga_preds))\n",
    "    idx_to_raga = {v: k for k, v in data_dict[\"raga_to_idx\"].items()}\n",
    "    used_raga_labels = [idx_to_raga[i] for i in used_raga_indices]\n",
    "\n",
    "    disp_raga = ConfusionMatrixDisplay(cm_raga, display_labels=used_raga_labels)\n",
    "    disp_raga.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Raga Confusion Matrix {prefix}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Compute accuracies\n",
    "    thaat_acc = accuracy_score(all_thaat_truth, all_thaat_preds)\n",
    "    raga_acc  = accuracy_score(all_raga_truth, all_raga_preds)\n",
    "    print(f\"{prefix} Thaat Accuracy: {thaat_acc:.4f}\")\n",
    "    print(f\"{prefix} Raga Accuracy:  {raga_acc:.4f}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training Curves {prefix}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save model\n",
    "    os.makedirs(\"data/modeloutput\", exist_ok=True)\n",
    "    if include_quantum:\n",
    "        checkpoint_path = os.path.join(\"data\", \"modeloutput\", \"trained_model_thaat_raga_quantum.pt\")\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(\"data\", \"modeloutput\", \"trained_model_thaat_raga_noquantum.pt\")\n",
    "\n",
    "    # Save relevant metadata\n",
    "    checkpoint_dict = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"thaat_to_idx\": data_dict[\"thaat_to_idx\"],\n",
    "        \"raga_to_idx\": data_dict[\"raga_to_idx\"],\n",
    "        \"scalar_dim\": scalar_feats.shape[1],\n",
    "        \"quantum_dim\": quantum_feats.shape[1],\n",
    "        \"num_thaats\": num_thaats,\n",
    "        \"num_ragas\": num_ragas,\n",
    "        \"used_ids\": used_ids\n",
    "    }\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "    print(f\"{prefix} Model + metadata saved to: {checkpoint_path}\")\n",
    "\n",
    "    return model, data_dict\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# COMPARISON FUNCTION: QUANTUM VS. NO-QUANTUM\n",
    "###############################################################################\n",
    "def compare_thaat_raga_models():\n",
    "    \"\"\"\n",
    "    1) Re-fetch the same dataset (using fetch_training_data_thaat_raga).\n",
    "    2) Build the same test set split (20%).\n",
    "    3) Load the quantum model checkpoint and the no-quantum model checkpoint.\n",
    "    4) Evaluate both on the *same test set*, compare thaat & raga accuracies.\n",
    "    \"\"\"\n",
    "    data_dict, used_ids = fetch_training_data_thaat_raga(limit=HYPERPARAMS[\"DB_LIMIT\"])\n",
    "    if data_dict is None:\n",
    "        print(\"[Compare] No data for comparison.\")\n",
    "        return\n",
    "\n",
    "    dataset_full = MultiLabelThaatRagaDataset(data_dict)\n",
    "    n_samples = len(dataset_full)\n",
    "    test_size = int(0.2 * n_samples)\n",
    "    train_size = n_samples - test_size\n",
    "    # We only want to evaluate on the test set. We do not re-train here.\n",
    "    _, test_ds = torch.utils.data.random_split(dataset_full, [train_size, test_size])\n",
    "    test_dl = DataLoader(test_ds, batch_size=HYPERPARAMS[\"BATCH_SIZE\"], shuffle=False)\n",
    "\n",
    "    # Load quantum model\n",
    "    q_ckpt_path = \"data/modeloutput/trained_model_thaat_raga_quantum.pt\"\n",
    "    if not os.path.exists(q_ckpt_path):\n",
    "        print(f\"[Compare] Quantum model not found at {q_ckpt_path}\")\n",
    "        return\n",
    "    q_ckpt = torch.load(q_ckpt_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    q_model = HybridASTModelThaatRaga(\n",
    "        num_thaats=q_ckpt[\"num_thaats\"],\n",
    "        num_ragas=q_ckpt[\"num_ragas\"],\n",
    "        scalar_dim=q_ckpt[\"scalar_dim\"],\n",
    "        quantum_dim=q_ckpt[\"quantum_dim\"],\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    q_model.load_state_dict(q_ckpt[\"model_state\"])\n",
    "    q_model.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    q_model.eval()\n",
    "\n",
    "    # Load no-quantum model\n",
    "    nq_ckpt_path = \"data/modeloutput/trained_model_thaat_raga_noquantum.pt\"\n",
    "    if not os.path.exists(nq_ckpt_path):\n",
    "        print(f\"[Compare] No-Quantum model not found at {nq_ckpt_path}\")\n",
    "        return\n",
    "    nq_ckpt = torch.load(nq_ckpt_path, map_location=HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "    nq_model = HybridASTModelThaatRagaNoQuantum(\n",
    "        num_thaats=nq_ckpt[\"num_thaats\"],\n",
    "        num_ragas=nq_ckpt[\"num_ragas\"],\n",
    "        scalar_dim=nq_ckpt[\"scalar_dim\"],\n",
    "        num_unfrozen_layers=HYPERPARAMS[\"NUM_AST_LAYERS_UNFROZEN\"]\n",
    "    )\n",
    "    nq_model.load_state_dict(nq_ckpt[\"model_state\"])\n",
    "    nq_model.to(HYPERPARAMS[\"DEVICE\"])\n",
    "    nq_model.eval()\n",
    "\n",
    "    # Evaluate both\n",
    "    all_true_thaats, all_true_ragas = [], []\n",
    "    preds_thaat_q, preds_raga_q = [], []\n",
    "    preds_thaat_nq, preds_raga_nq = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel_img, scal, quan, t_lbl, r_lbl in test_dl:\n",
    "            mel_img = mel_img.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            scal    = scal.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            quan    = quan.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            t_lbl   = t_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "            r_lbl   = r_lbl.to(HYPERPARAMS[\"DEVICE\"])\n",
    "\n",
    "            # Quantum\n",
    "            logits_thaat_q, logits_raga_q = q_model(mel_img, scal, quan)\n",
    "            pred_t_q = logits_thaat_q.argmax(dim=1).cpu().numpy()\n",
    "            pred_r_q = logits_raga_q.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            # NoQuantum\n",
    "            logits_thaat_nq, logits_raga_nq = nq_model(mel_img, scal)\n",
    "            pred_t_nq = logits_thaat_nq.argmax(dim=1).cpu().numpy()\n",
    "            pred_r_nq = logits_raga_nq.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            all_true_thaats.extend(t_lbl.cpu().numpy())\n",
    "            all_true_ragas.extend(r_lbl.cpu().numpy())\n",
    "            preds_thaat_q.extend(pred_t_q)\n",
    "            preds_raga_q.extend(pred_r_q)\n",
    "            preds_thaat_nq.extend(pred_t_nq)\n",
    "            preds_raga_nq.extend(pred_r_nq)\n",
    "\n",
    "    # Accuracy\n",
    "    thaat_acc_q  = accuracy_score(all_true_thaats, preds_thaat_q)\n",
    "    thaat_acc_nq = accuracy_score(all_true_thaats, preds_thaat_nq)\n",
    "    raga_acc_q   = accuracy_score(all_true_ragas, preds_raga_q)\n",
    "    raga_acc_nq  = accuracy_score(all_true_ragas, preds_raga_nq)\n",
    "\n",
    "    print(\"\\n=== Thaat-Raga: Quantum vs. No-Quantum ===\")\n",
    "    print(f\"Thaat Accuracy (Quantum):     {thaat_acc_q:.4f}\")\n",
    "    print(f\"Thaat Accuracy (No-Quantum):  {thaat_acc_nq:.4f}\")\n",
    "    print(f\"Raga  Accuracy (Quantum):     {raga_acc_q:.4f}\")\n",
    "    print(f\"Raga  Accuracy (No-Quantum):  {raga_acc_nq:.4f}\")\n",
    "\n",
    "    d_thaat = (thaat_acc_q - thaat_acc_nq) * 100\n",
    "    d_raga  = (raga_acc_q - raga_acc_nq) * 100\n",
    "    print(f\"Difference in Thaat Accuracy = {d_thaat:.2f} % points\")\n",
    "    print(f\"Difference in Raga Accuracy  = {d_raga:.2f} % points\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE MAIN\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Train with quantum\n",
    "    model_q, data_q = train_thaat_raga_model(include_quantum=True)\n",
    "\n",
    "    # 2) Train no-quantum\n",
    "    model_nq, data_nq = train_thaat_raga_model(include_quantum=False)\n",
    "\n",
    "    # 3) Compare\n",
    "    compare_thaat_raga_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
