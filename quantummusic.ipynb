{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {},
      "source": [
        "# QuantumMusic - Unified Version with Larger LUFS Chunk\n",
        "\n",
        "### Introduction\n",
        "This notebook merges:\n",
        "- **Module 7: Dynamics (RMS + LUFS)** with a **longer** chunk size for loudness calculations.\n",
        "- **Multi‐chunk tempo** (512, 4096, 16384).\n",
        "\n",
        "We do **not** remove any docstrings or modules. We have introduced a new constant `LUFS_CHUNK_SIZE` in **Module 7**. **Additionally**, we have added short‐chunk fixes (`n_fft_local = min(...)`) so Librosa won't emit warnings about chunk length < `n_fft`.\n",
        "\n",
        "Author: Rohan Agarwal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "constants",
      "metadata": {},
      "source": [
        "## 1. Constants & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-constants",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. CONSTANTS & IMPORTS\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import psycopg2\n",
        "from psycopg2.extras import Json\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "# For Praat-based HNR\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "\n",
        "# For audio playback in Jupyter\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# For LUFS-based loudness\n",
        "try:\n",
        "    import pyloudnorm as pyln\n",
        "    LOUDNORM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LOUDNORM_AVAILABLE = False\n",
        "    print(\"Warning: pyloudnorm not installed, LUFS computations will be skipped.\")\n",
        "\n",
        "# Directory constants\n",
        "INPUT_DIR = \"input\"\n",
        "OUTPUT_DIR = \"output\"\n",
        "\n",
        "# Database constants\n",
        "DB_NAME = \"quantummusic\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_USER = \"postgres\"  # placeholder\n",
        "DB_PASSWORD = \"postgres\"  # placeholder\n",
        "\n",
        "# Audio processing constants\n",
        "STANDARD_SR = 44100  # Standard sampling rate\n",
        "SILENCE_THRESHOLD_DB = 30  # dB threshold for silence trimming\n",
        "\n",
        "# Band-pass filter constants\n",
        "LOW_FREQ = 80.0\n",
        "HIGH_FREQ = 3000.0\n",
        "\n",
        "# Visualization constants\n",
        "FIG_SIZE = (10, 4)\n",
        "\n",
        "# Save-to-DB constant\n",
        "SAVE_TO_DB = True\n",
        "\n",
        "# Frame-based approach for pitch detection\n",
        "FRAME_SIZE = 2048\n",
        "HOP_LENGTH = 512  # normal frames\n",
        "# Praat-based chunk size\n",
        "PRAAT_CHUNK_SIZE = 2048  # for Praat\n",
        "\n",
        "# Deviation threshold in cents, for dev_flag\n",
        "DEVIATION_THRESHOLD = 50.0\n",
        "\n",
        "# Multi-chunk tempo analysis\n",
        "TEMPO_CHUNK_SIZE_MEDIUM = 4096\n",
        "TEMPO_CHUNK_SIZE_LARGE = 16384\n",
        "\n",
        "# NEW constant for LUFS calculations (0.5s @ 44.1kHz)\n",
        "LUFS_CHUNK_SIZE = 22050\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def test_imports():\n",
        "    print(\"Imports tested, all modules are available.\")\n",
        "\n",
        "test_imports()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25acf27a",
      "metadata": {},
      "source": [
        "## 1.1 - Converting Numbers to Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f4c12516",
      "metadata": {},
      "outputs": [],
      "source": [
        "########################\n",
        "# NEW: Classification Helpers\n",
        "########################\n",
        "\n",
        "def classify_pitch_deviation(dev_cents):\n",
        "    \"\"\"\n",
        "    Classify pitch deviation into categories based on absolute distance from nearest note.\n",
        "    \"\"\"\n",
        "    if dev_cents is None or np.isnan(dev_cents):\n",
        "        return \"unknown\"\n",
        "    dev_abs = abs(dev_cents)\n",
        "    if dev_abs < 10:\n",
        "        return \"perfect\"\n",
        "    elif dev_abs < 30:\n",
        "        return \"good\"\n",
        "    elif dev_abs < 50:\n",
        "        return \"fair\"\n",
        "    elif dev_abs < 100:\n",
        "        return \"poor\"\n",
        "    else:\n",
        "        return \"very poor\"\n",
        "\n",
        "\n",
        "def classify_tone_to_noise(sf_val):\n",
        "    \"\"\"\n",
        "    Classify spectral flatness into a rough 'tonal vs. noisy' scale.\n",
        "    Typical spectral flatness range is ~0 to 1.\n",
        "    \"\"\"\n",
        "    if np.isnan(sf_val):\n",
        "        return \"unknown\"\n",
        "    if sf_val < 0.02:\n",
        "        return \"very tonal\"\n",
        "    elif sf_val < 0.08:\n",
        "        return \"mostly tonal\"\n",
        "    elif sf_val < 0.15:\n",
        "        return \"mixed\"\n",
        "    elif sf_val < 0.3:\n",
        "        return \"noisy\"\n",
        "    else:\n",
        "        return \"very noisy\"\n",
        "\n",
        "\n",
        "def classify_transition_score(score):\n",
        "    \"\"\"\n",
        "    Classify how smoothly the pitch transitions from the previous chunk.\n",
        "    Score ~ 0..1, with 1 = no pitch jump, 0 = extremely large jump\n",
        "    \"\"\"\n",
        "    if np.isnan(score):\n",
        "        return \"unknown\"\n",
        "    if score >= 0.8:\n",
        "        return \"smooth\"\n",
        "    elif score >= 0.5:\n",
        "        return \"moderate\"\n",
        "    else:\n",
        "        return \"abrupt\"\n",
        "\n",
        "\n",
        "def classify_rms_db(rms_db):\n",
        "    \"\"\"\n",
        "    Classify RMS in dB into categories from very soft => very loud\n",
        "    \"\"\"\n",
        "    if np.isnan(rms_db):\n",
        "        return \"unknown\"\n",
        "    if rms_db < -40:\n",
        "        return \"very soft\"\n",
        "    elif rms_db < -20:\n",
        "        return \"soft\"\n",
        "    elif rms_db < -10:\n",
        "        return \"moderate\"\n",
        "    elif rms_db < -2:\n",
        "        return \"loud\"\n",
        "    else:\n",
        "        return \"very loud\"\n",
        "\n",
        "\n",
        "def classify_lufs(lufs_val):\n",
        "    \"\"\"\n",
        "    Classify LUFS values into rough loudness categories.\n",
        "    Typically, lower (more negative) = quieter, higher = louder.\n",
        "    \"\"\"\n",
        "    if lufs_val is None or np.isnan(lufs_val):\n",
        "        return \"unknown\"\n",
        "    if lufs_val < -40:\n",
        "        return \"very soft\"\n",
        "    elif lufs_val < -23:\n",
        "        return \"soft\"\n",
        "    elif lufs_val < -14:\n",
        "        return \"moderate\"\n",
        "    elif lufs_val < -5:\n",
        "        return \"loud\"\n",
        "    else:\n",
        "        return \"very loud\"\n",
        "\n",
        "\n",
        "def classify_praat_hnr(hnr_val):\n",
        "    \"\"\"\n",
        "    Classify Praat HNR (dB). Typical range: 0..35 dB for normal voices.\n",
        "    Higher = more harmonic, lower = more noise.\n",
        "    \"\"\"\n",
        "    if np.isnan(hnr_val):\n",
        "        return \"unknown\"\n",
        "    if hnr_val < 5:\n",
        "        return \"very noisy\"\n",
        "    elif hnr_val < 15:\n",
        "        return \"noisy\"\n",
        "    elif hnr_val < 25:\n",
        "        return \"moderately harmonic\"\n",
        "    else:\n",
        "        return \"very harmonic\"\n",
        "\n",
        "\n",
        "def classify_tempo_bpm(bpm_val):\n",
        "    \"\"\"\n",
        "    Classify BPM values into slow, moderate, fast, etc.\n",
        "    \"\"\"\n",
        "    if np.isnan(bpm_val):\n",
        "        return \"unknown\"\n",
        "    if bpm_val < 40:\n",
        "        return \"very slow\"\n",
        "    elif bpm_val < 70:\n",
        "        return \"slow\"\n",
        "    elif bpm_val < 110:\n",
        "        return \"moderate\"\n",
        "    elif bpm_val < 160:\n",
        "        return \"fast\"\n",
        "    else:\n",
        "        return \"very fast\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db-scaffolding",
      "metadata": {},
      "source": [
        "## 2. Database Scaffolding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "code-database",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. DATABASE SCAFFOLDING\n",
        "\n",
        "class QuantumMusicDB:\n",
        "    \"\"\"\n",
        "    Handles connection to the PostgreSQL database and basic CRUD operations.\n",
        "    \"\"\"\n",
        "    def __init__(self, db_name=DB_NAME, host=DB_HOST, user=DB_USER, password=DB_PASSWORD):\n",
        "        self.db_name = db_name\n",
        "        self.host = host\n",
        "        self.user = user\n",
        "        self.password = password\n",
        "        self.conn = None\n",
        "        self.connect()\n",
        "\n",
        "    def connect(self):\n",
        "        try:\n",
        "            self.conn = psycopg2.connect(\n",
        "                dbname=self.db_name,\n",
        "                host=self.host,\n",
        "                user=self.user,\n",
        "                password=self.password\n",
        "            )\n",
        "            print(f\"Connected to database {self.db_name} successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to database: {e}\")\n",
        "\n",
        "    def create_tables(self):\n",
        "        create_table_query = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS audio_analysis (\n",
        "            id SERIAL PRIMARY KEY,\n",
        "            file_name VARCHAR(255),\n",
        "            upload_date TIMESTAMP DEFAULT NOW(),\n",
        "            sample_rate INT,\n",
        "            analysis_data JSONB\n",
        "        );\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(create_table_query)\n",
        "            self.conn.commit()\n",
        "        print(\"Tables ensured.\")\n",
        "\n",
        "    def insert_analysis(self, file_name, sample_rate, analysis_data):\n",
        "        \"\"\"\n",
        "        Insert a new analysis record into the DB.\n",
        "        analysis_data is stored as a JSONB column using psycopg2.extras.Json.\n",
        "        \"\"\"\n",
        "        insert_query = \"\"\"\n",
        "        INSERT INTO audio_analysis(file_name, sample_rate, analysis_data)\n",
        "        VALUES (%s, %s, %s)\n",
        "        RETURNING id;\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(insert_query, (file_name, sample_rate, Json(analysis_data)))\n",
        "            new_id = cur.fetchone()[0]\n",
        "            self.conn.commit()\n",
        "        return new_id\n",
        "\n",
        "    def fetch_analysis(self, record_id):\n",
        "        \"\"\"\n",
        "        Fetch a specific analysis record by ID.\n",
        "        \"\"\"\n",
        "        select_query = \"\"\"\n",
        "        SELECT id, file_name, sample_rate, analysis_data\n",
        "        FROM audio_analysis\n",
        "        WHERE id=%s;\n",
        "        \"\"\"\n",
        "        with self.conn.cursor() as cur:\n",
        "            cur.execute(select_query, (record_id,))\n",
        "            row = cur.fetchone()\n",
        "        return row\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "            print(\"Database connection closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preprocessing-mod",
      "metadata": {},
      "source": [
        "## 3. Module 1: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "preprocessing-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. MODULE 1: PREPROCESSING\n",
        "\n",
        "def preprocess_audio(file_path,\n",
        "                     target_sr=STANDARD_SR,\n",
        "                     silence_threshold_db=SILENCE_THRESHOLD_DB):\n",
        "    audio_data, sr = librosa.load(file_path, sr=None)\n",
        "    if sr != target_sr:\n",
        "        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=target_sr)\n",
        "        sr = target_sr\n",
        "\n",
        "    peak = np.max(np.abs(audio_data))\n",
        "    if peak > 0:\n",
        "        audio_data = audio_data / peak\n",
        "\n",
        "    audio_data, _ = librosa.effects.trim(audio_data, top_db=silence_threshold_db)\n",
        "    return audio_data, sr\n",
        "\n",
        "\n",
        "\n",
        "def remove_percussive_components(audio_data, sr, margin=(1.0, 1.0)):\n",
        "    \"\"\"\n",
        "    Uses harmonic-percussive source separation (HPSS) to split audio into harmonic and percussive components.\n",
        "    By default, returns only the harmonic component (vocals, sustained instruments).\n",
        "    \n",
        "    :param audio_data: np.array containing the audio samples\n",
        "    :param sr: sampling rate\n",
        "    :param margin: A tuple specifying HPSS margin for harmonic/percussive. \n",
        "                   Larger margins can better isolate percussive vs. harmonic content, \n",
        "                   but it may remove more from the harmonic portion.\n",
        "    :return: np.array containing only the harmonic component\n",
        "    \"\"\"\n",
        "    # Perform HPSS with Librosa\n",
        "    # margin=(harmonic_margin, percussive_margin)\n",
        "    # Adjust margins if you have a strongly percussive track\n",
        "    harmonic_part, percussive_part = librosa.decompose.hpss(\n",
        "        librosa.stft(audio_data), \n",
        "        margin=margin\n",
        "    )\n",
        "    # Convert back from STFT to time-domain\n",
        "    harmonic_audio = librosa.istft(harmonic_part)\n",
        "    \n",
        "    return harmonic_audio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noise-mod",
      "metadata": {},
      "source": [
        "## 4. Module 2: Noise Identification & Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "noise-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. MODULE 2: NOISE IDENTIFICATION & REDUCTION\n",
        "\n",
        "def bandpass_filter(audio_data, sr, low_freq=LOW_FREQ, high_freq=HIGH_FREQ):\n",
        "    nyquist = 0.5 * sr\n",
        "    low = low_freq / nyquist\n",
        "    high = high_freq / nyquist\n",
        "    b, a = butter(N=4, Wn=[low, high], btype='band')\n",
        "    filtered_audio = filtfilt(b, a, audio_data)\n",
        "    return filtered_audio\n",
        "\n",
        "def simple_noise_reduction(audio_data, sr):\n",
        "    filtered = bandpass_filter(audio_data, sr)\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pitch-mod",
      "metadata": {},
      "source": [
        "## 5. Module 3: Pitch Contour Extraction and Analysis\n",
        "\n",
        "### Key Points:\n",
        "- We define `extract_pitch_contour` to detect pitch frames with `librosa.pyin`.\n",
        "- We define `freq_to_closest_note` and `map_pitches_to_notes` to map a pitch to the nearest 12-tone note.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "pitch-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "NOTE_NAMES = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
        "\n",
        "def extract_pitch_contour(audio_data, sr):\n",
        "    \"\"\"\n",
        "    Uses librosa.pyin to extract pitch contour.\n",
        "    Returns (times, pitches, confidences).\n",
        "    \"\"\"\n",
        "    fmin = librosa.note_to_hz('C2')\n",
        "    fmax = librosa.note_to_hz('C7')\n",
        "    # Check if audio_data is stereo, if so, take the mean?\n",
        "    # TODO: Check if I want to take audio_data or harmonic_data here?\n",
        "    pitches, voiced_flags, confidences = librosa.pyin(\n",
        "        y=audio_data,\n",
        "        fmin=fmin,\n",
        "        fmax=fmax,\n",
        "        sr=sr,\n",
        "        frame_length=FRAME_SIZE,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    times = librosa.times_like(pitches, sr=sr, hop_length=HOP_LENGTH)\n",
        "    return times, pitches, confidences\n",
        "\n",
        "def freq_to_closest_note(freq):\n",
        "    \"\"\"\n",
        "    Convert freq in Hz to the nearest semitone and compute the difference in cents.\n",
        "    Returns (note_name, note_freq, deviation_cents) or (None,None,None) if invalid.\n",
        "    \"\"\"\n",
        "    if freq is None or freq <= 0 or np.isnan(freq):\n",
        "        return (None, None, None)\n",
        "    note_num = 69 + 12 * np.log2(freq / 440.0)\n",
        "    if np.isnan(note_num) or np.isinf(note_num):\n",
        "        return (None, None, None)\n",
        "    rounded_note = int(round(note_num))\n",
        "    closest_note_freq = 440.0 * (2.0 ** ((rounded_note - 69)/12.0))\n",
        "    deviation_cents = 1200 * np.log2(freq / closest_note_freq)\n",
        "    note_name = NOTE_NAMES[rounded_note % 12]\n",
        "    octave = (rounded_note // 12) - 1\n",
        "    full_note_name = f\"{note_name}{octave}\"\n",
        "    return (full_note_name, closest_note_freq, deviation_cents)\n",
        "\n",
        "def map_pitches_to_notes(pitches):\n",
        "    \"\"\"\n",
        "    For an array of pitch frames, map each to (note_name, note_freq, deviation_cents).\n",
        "    Return arrays: note_names, note_freqs, deviations.\n",
        "    \"\"\"\n",
        "    note_names = []\n",
        "    note_freqs = []\n",
        "    deviations = []\n",
        "    for p in pitches:\n",
        "        name, freq, dev = freq_to_closest_note(p)\n",
        "        note_names.append(name)\n",
        "        note_freqs.append(freq)\n",
        "        deviations.append(dev)\n",
        "    return note_names, note_freqs, deviations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "praat-hnr",
      "metadata": {},
      "source": [
        "## 6. Utility for Praat HNR (larger chunk), plus separate time_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "praat-hnr-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_praat_hnr(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    required_length_sec = 3.0 / 86.1329  # ~0.0348 s\n",
        "    required_length_samples = int(required_length_sec * sr)\n",
        "    if len(audio_chunk) < required_length_samples:\n",
        "        return 0.0\n",
        "\n",
        "    try:\n",
        "        sound = parselmouth.Sound(audio_chunk, sr)\n",
        "        harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, 86.1329, 0.1, 1.0)\n",
        "        hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
        "        if np.isnan(hnr) or np.isinf(hnr):\n",
        "            return 0.0\n",
        "        return float(hnr)\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "def build_praat_time_matrix(audio_data, sr):\n",
        "    matrix = []\n",
        "    num_samples = len(audio_data)\n",
        "    idx = 0\n",
        "    chunk_counter = 0\n",
        "    chunk_hop = PRAAT_CHUNK_SIZE\n",
        "\n",
        "    while idx < num_samples:\n",
        "        end = idx + chunk_hop\n",
        "        if end > num_samples:\n",
        "            end = num_samples\n",
        "        chunk_data = audio_data[idx:end]\n",
        "        start_time = idx / sr\n",
        "        hnr_value = compute_praat_hnr(chunk_data, sr)\n",
        "\n",
        "        matrix.append({\n",
        "            \"chunk_index\": chunk_counter,\n",
        "            \"start_time_s\": float(start_time),\n",
        "            \"praat_hnr\": hnr_value\n",
        "        })\n",
        "\n",
        "        idx += chunk_hop\n",
        "        chunk_counter += 1\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod7-dynamics-again",
      "metadata": {},
      "source": [
        "## 7. Module 7: Dynamics (RMS & LUFS) with Larger Chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "dynamics-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_rms_energy_advanced(audio_chunk):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    n_fft_local = min(len(audio_chunk), LUFS_CHUNK_SIZE)\n",
        "    rms = librosa.feature.rms(\n",
        "        y=audio_chunk,\n",
        "        frame_length=n_fft_local,\n",
        "        hop_length=n_fft_local\n",
        "    )\n",
        "    return float(rms.mean())\n",
        "\n",
        "def compute_lufs(audio_chunk, sr):\n",
        "    if not LOUDNORM_AVAILABLE:\n",
        "        return None\n",
        "    if len(audio_chunk) == 0:\n",
        "        return None\n",
        "\n",
        "    # Convert to mono if multi-channel\n",
        "    if audio_chunk.ndim > 1:\n",
        "        audio_chunk = np.mean(audio_chunk, axis=1)\n",
        "\n",
        "    # Make sure chunk is long enough for integrated loudness:\n",
        "    meter = pyln.Meter(sr)  # by default block_size=0.4\n",
        "    min_required = int(meter.block_size * sr)  # typically 0.4 * 44100 = 17640\n",
        "    if len(audio_chunk) < min_required:\n",
        "        return 0.0\n",
        "\n",
        "    loudness_val = meter.integrated_loudness(audio_chunk)\n",
        "    return float(loudness_val)\n",
        "\n",
        "def analyze_dynamics_module7(audio_data, sr, time_matrix):\n",
        "    \"\"\"\n",
        "    Now we use LUFS_CHUNK_SIZE (~0.5s) for integrated loudness.\n",
        "    For each row in time_matrix, we gather the start_time and read ~0.5s of audio.\n",
        "    Then store RMS(dB) and LUFS.\n",
        "    Also store summary in the final row.\n",
        "    \"\"\"\n",
        "    rms_values_db = []\n",
        "    lufs_values = []\n",
        "\n",
        "    for i, row in enumerate(time_matrix):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + LUFS_CHUNK_SIZE\n",
        "        if end_sample > len(audio_data):\n",
        "            end_sample = len(audio_data)\n",
        "        chunk_data = audio_data[start_sample:end_sample]\n",
        "        # TODO: Grab harmonic data instead?\n",
        "\n",
        "        raw_rms = compute_rms_energy_advanced(chunk_data)\n",
        "        rms_db = 20.0 * np.log10(raw_rms + 1e-12)\n",
        "        row['rms_db'] = rms_db\n",
        "        ################################\n",
        "        # NEW: Store RMS category\n",
        "        ################################\n",
        "        row['rms_db_category'] = classify_rms_db(rms_db)\n",
        "        rms_values_db.append(rms_db)\n",
        "\n",
        "        lufs_val = compute_lufs(chunk_data, sr)\n",
        "        if lufs_val is None:\n",
        "            lufs_val = 0.0\n",
        "        row['lufs'] = lufs_val\n",
        "        ################################\n",
        "        # NEW: Store LUFS category\n",
        "        ################################\n",
        "        row['lufs_category'] = classify_lufs(lufs_val)\n",
        "\n",
        "        lufs_values.append(lufs_val)\n",
        "\n",
        "    # Summaries\n",
        "    valid_rms = [x for x in rms_values_db if not np.isnan(x) and not np.isinf(x)]\n",
        "    if valid_rms:\n",
        "        mean_rms = float(np.mean(valid_rms))\n",
        "        med_rms = float(np.median(valid_rms))\n",
        "        min_rms = float(np.min(valid_rms))\n",
        "        max_rms = float(np.max(valid_rms))\n",
        "        range_rms = max_rms - min_rms\n",
        "        std_rms = float(np.std(valid_rms))\n",
        "        dyn_range_rms = range_rms\n",
        "    else:\n",
        "        mean_rms = med_rms = min_rms = max_rms = range_rms = std_rms = dyn_range_rms = 0.0\n",
        "\n",
        "    valid_lufs = [x for x in lufs_values if not np.isnan(x) and not np.isinf(x)]\n",
        "    if valid_lufs:\n",
        "        mean_lufs = float(np.mean(valid_lufs))\n",
        "        med_lufs = float(np.median(valid_lufs))\n",
        "        min_lufs = float(np.min(valid_lufs))\n",
        "        max_lufs = float(np.max(valid_lufs))\n",
        "        range_lufs = max_lufs - min_lufs\n",
        "        std_lufs = float(np.std(valid_lufs))\n",
        "        dyn_range_lufs = range_lufs\n",
        "    else:\n",
        "        mean_lufs = med_lufs = min_lufs = max_lufs = range_lufs = std_lufs = dyn_range_lufs = 0.0\n",
        "\n",
        "    dyn_summary = {\n",
        "        'rms_db': {\n",
        "            'mean': mean_rms,\n",
        "            'median': med_rms,\n",
        "            'min': min_rms,\n",
        "            'max': max_rms,\n",
        "            'range': range_rms,\n",
        "            'std': std_rms,\n",
        "            'dynamic_range': dyn_range_rms\n",
        "        },\n",
        "        'lufs': {\n",
        "            'mean': mean_lufs,\n",
        "            'median': med_lufs,\n",
        "            'min': min_lufs,\n",
        "            'max': max_lufs,\n",
        "            'range': range_lufs,\n",
        "            'std': std_lufs,\n",
        "            'dynamic_range': dyn_range_lufs\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if time_matrix:\n",
        "        time_matrix[-1]['dynamics_summary'] = dyn_summary\n",
        "\n",
        "    return time_matrix, dyn_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8-analyze-transitions",
      "metadata": {},
      "source": [
        "## 8. Normal time_matrix analysis (tone_to_noise, transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "transitions-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_spectral_flatness_chunk(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    # Short-chunk fix:\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    sf = librosa.feature.spectral_flatness(\n",
        "        y=audio_chunk,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(np.mean(sf))\n",
        "\n",
        "def analyze_note_transitions(audio_data, sr, time_matrix):\n",
        "    for i, row in enumerate(time_matrix):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + HOP_LENGTH\n",
        "        if end_sample > len(audio_data):\n",
        "            end_sample = len(audio_data)\n",
        "        chunk_data = audio_data[start_sample:end_sample]\n",
        "\n",
        "        # tone_to_noise => spectral flatness\n",
        "        sf_val = compute_spectral_flatness_chunk(chunk_data, sr)\n",
        "        row['tone_to_noise'] = sf_val\n",
        "        ################################\n",
        "        # NEW: Store tone category\n",
        "        ################################\n",
        "        row['tone_to_noise_category'] = classify_tone_to_noise(sf_val)\n",
        "\n",
        "\n",
        "        # transition_score => pitch difference\n",
        "        if i == 0:\n",
        "            transition_score = 1.0\n",
        "        else:\n",
        "            prev_pitch = time_matrix[i-1].get('pitch_hz', None)\n",
        "            curr_pitch = row.get('pitch_hz', None)\n",
        "            if not prev_pitch or not curr_pitch or prev_pitch <= 0 or curr_pitch <= 0:\n",
        "                transition_score = 0.5\n",
        "            else:\n",
        "                diff_cents = 1200.0 * np.log2(curr_pitch / prev_pitch)\n",
        "                diff_abs = abs(diff_cents)\n",
        "                transition_score = max(0.0, 1.0 - diff_abs/200.0)\n",
        "        row['transition_score'] = transition_score\n",
        "        ################################\n",
        "        # NEW: Store transition category\n",
        "        ################################\n",
        "        row['transition_category'] = classify_transition_score(transition_score)\n",
        "\n",
        "\n",
        "    return time_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9-spectrogram",
      "metadata": {},
      "source": [
        "## 9. Spectrogram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "spectrogram-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_zero_crossing_rate(audio_chunk):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    # Short-chunk fix:\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    zcr = librosa.feature.zero_crossing_rate(\n",
        "        y=audio_chunk,\n",
        "        frame_length=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(zcr.mean())\n",
        "\n",
        "def compute_spectral_centroid_advanced(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    c = librosa.feature.spectral_centroid(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(c.mean())\n",
        "\n",
        "def compute_spectral_rolloff_advanced(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    r = librosa.feature.spectral_rolloff(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(r.mean())\n",
        "\n",
        "def compute_spectral_bandwidth_advanced(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return 0.0\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    bw = librosa.feature.spectral_bandwidth(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return float(bw.mean())\n",
        "\n",
        "def compute_mfccs_advanced(audio_chunk, sr, n_mfcc=60):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return [0.0]*n_mfcc\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    mfcc_data = librosa.feature.mfcc(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_mfcc=n_mfcc,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        fmax=sr/2\n",
        "    )\n",
        "    return mfcc_data.mean(axis=1).tolist()\n",
        "\n",
        "def compute_chroma_advanced(audio_chunk, sr):\n",
        "    if len(audio_chunk) == 0:\n",
        "        return [0.0]*12\n",
        "    n_fft_local = min(len(audio_chunk), FRAME_SIZE)\n",
        "    c = librosa.feature.chroma_stft(\n",
        "        y=audio_chunk,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft_local,\n",
        "        hop_length=HOP_LENGTH\n",
        "    )\n",
        "    return c.mean(axis=1).tolist()\n",
        "\n",
        "def update_time_matrix_small_with_advanced_features(clean_audio, sr, time_matrix_small):\n",
        "    \"\"\"\n",
        "    For each chunk in time_matrix_small (512-hop), compute advanced spectral features:\n",
        "    ZCR, centroid, rolloff, bandwidth, MFCC, Chroma.\n",
        "    We do not do RMS here because Module 7 handles RMS with a bigger chunk size.\n",
        "    \"\"\"\n",
        "    for row in time_matrix_small:\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + HOP_LENGTH\n",
        "        if end_sample > len(clean_audio):\n",
        "            end_sample = len(clean_audio)\n",
        "        chunk_data = clean_audio[start_sample:end_sample]\n",
        "\n",
        "        if not np.any(chunk_data):\n",
        "            row['zcr'] = 0.0\n",
        "            row['spec_centroid'] = 0.0\n",
        "            row['spec_rolloff'] = 0.0\n",
        "            row['spec_bandwidth'] = 0.0\n",
        "            row['mfcc'] = [0.0]*60\n",
        "            row['chroma'] = [0.0]*12\n",
        "            continue\n",
        "\n",
        "        row['zcr'] = compute_zero_crossing_rate(chunk_data)\n",
        "        row['spec_centroid'] = compute_spectral_centroid_advanced(chunk_data, sr)\n",
        "        row['spec_rolloff'] = compute_spectral_rolloff_advanced(chunk_data, sr)\n",
        "        row['spec_bandwidth'] = compute_spectral_bandwidth_advanced(chunk_data, sr)\n",
        "        row['mfcc'] = compute_mfccs_advanced(chunk_data, sr, 60)\n",
        "        row['chroma'] = compute_chroma_advanced(chunk_data, sr)\n",
        "\n",
        "    return time_matrix_small\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod10-tempo",
      "metadata": {},
      "source": [
        "## 10. Tempo Adherence (512‐hop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "tempo512-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_tempo_adherence(clean_audio, sr, time_matrix_small):\n",
        "    \"\"\"\n",
        "    For each chunk in time_matrix_small, estimate local tempo (in BPM)\n",
        "    and store it as row['tempo_bpm']. Uses 512-sample frames.\n",
        "    \"\"\"\n",
        "    import librosa\n",
        "    from librosa.feature import rhythm\n",
        "\n",
        "    for i, row in enumerate(time_matrix_small):\n",
        "        start_time = row['time_s']\n",
        "        start_sample = int(start_time * sr)\n",
        "        end_sample = start_sample + HOP_LENGTH\n",
        "        if end_sample > len(clean_audio):\n",
        "            end_sample = len(clean_audio)\n",
        "        chunk_data = clean_audio[start_sample:end_sample]\n",
        "\n",
        "        if len(chunk_data) == 0 or not np.any(chunk_data):\n",
        "            row['tempo_bpm'] = 0.0\n",
        "            ################################\n",
        "            # NEW: Store tempo category\n",
        "            ################################\n",
        "            row['tempo_bpm_category'] = \"unknown\"\n",
        "            continue\n",
        "\n",
        "        local_tempo = rhythm.tempo(\n",
        "            y=chunk_data,\n",
        "            sr=sr,\n",
        "            hop_length=HOP_LENGTH,\n",
        "            aggregate=None\n",
        "        )\n",
        "        if (local_tempo is None) or (len(local_tempo) == 0):\n",
        "            row['tempo_bpm'] = 0.0\n",
        "            row['tempo_bpm_category'] = \"unknown\"\n",
        "        else:\n",
        "            ################################\n",
        "            # NEW: Store tempo category\n",
        "            ################################\n",
        "            tempo_val = float(np.mean(local_tempo))\n",
        "            row['tempo_bpm'] = tempo_val\n",
        "            row['tempo_bpm_category'] = classify_tempo_bpm(tempo_val)\n",
        "\n",
        "\n",
        "    return time_matrix_small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod10b-tempo",
      "metadata": {},
      "source": [
        "## 10b. Additional Multi‐chunk Tempo (4096, 16384)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "tempo-larger-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_larger_tempo_matrix(clean_audio, sr, chunk_size=4096, overlap=0.5):\n",
        "    \"\"\"\n",
        "    Build a separate time matrix for tempo analysis using a larger chunk.\n",
        "    e.g. chunk_size=4096 or 16384, with default 50% overlap => hop=chunk_size/2.\n",
        "    \"\"\"\n",
        "    import librosa\n",
        "    from librosa.feature import rhythm\n",
        "\n",
        "    matrix = []\n",
        "    num_samples = len(clean_audio)\n",
        "    chunk_hop = int(chunk_size * (1.0 - overlap))\n",
        "    idx = 0\n",
        "    chunk_counter = 0\n",
        "\n",
        "    while idx < num_samples:\n",
        "        end = idx + chunk_size\n",
        "        if end > num_samples:\n",
        "            end = num_samples\n",
        "        chunk_data = clean_audio[idx:end]\n",
        "        start_time = idx / sr\n",
        "\n",
        "        if len(chunk_data) == 0 or not np.any(chunk_data):\n",
        "            tempo_bpm = 0.0\n",
        "            tempo_bpm_category = \"unknown\"\n",
        "\n",
        "        else:\n",
        "            local_tempo = rhythm.tempo(\n",
        "                y=chunk_data,\n",
        "                sr=sr,\n",
        "                hop_length=HOP_LENGTH,\n",
        "                aggregate=None\n",
        "            )\n",
        "            if (local_tempo is None) or (len(local_tempo) == 0):\n",
        "                tempo_bpm = 0.0\n",
        "                tempo_bpm_category = \"unknown\"\n",
        "\n",
        "            else:\n",
        "                tempo_bpm = float(np.mean(local_tempo))\n",
        "                ################################\n",
        "                # NEW: Store tempo category\n",
        "                ################################\n",
        "                tempo_bpm_category = classify_tempo_bpm(tempo_bpm)\n",
        "\n",
        "\n",
        "        matrix.append({\n",
        "            \"chunk_index\": chunk_counter,\n",
        "            \"start_time_s\": float(start_time),\n",
        "            \"tempo_bpm\": tempo_bpm,\n",
        "            \"tempo_bpm_category\": tempo_bpm_category  # <--- NEW\n",
        "\n",
        "        })\n",
        "\n",
        "        idx += chunk_hop\n",
        "        chunk_counter += 1\n",
        "\n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod11-plot",
      "metadata": {},
      "source": [
        "## 11. Plotting All Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "plot-all-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_all_metrics(\n",
        "    time_matrix_small,\n",
        "    praat_matrix,\n",
        "    audio_data=None,\n",
        "    sr=None,\n",
        "    time_matrix_tempo_medium=None,\n",
        "    time_matrix_tempo_large=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot pitch, transitions, HNR, advanced features, plus RMS & LUFS (with new chunk size)\n",
        "    and multi-chunk tempo lines.\n",
        "    \"\"\"\n",
        "    import librosa.display\n",
        "\n",
        "    # Figure 1: pitch, spectral flatness, transition, Praat HNR\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=False)\n",
        "\n",
        "    # Subplot 1: pitch & note freq\n",
        "    ax0 = axes[0]\n",
        "    times_small = [row.get('time_s', 0.0) for row in time_matrix_small]\n",
        "    pitch_small = [row.get('pitch_hz', 0.0) for row in time_matrix_small]\n",
        "    note_freq_small = [row.get('note_freq_hz', 0.0) for row in time_matrix_small]\n",
        "    ax0.plot(times_small, pitch_small, label='Pitch (Hz)', color='b')\n",
        "    ax0.plot(times_small, note_freq_small, label='NoteFreq (Hz)', color='orange', alpha=0.7)\n",
        "    ax0.set_ylabel('Frequency (Hz)')\n",
        "    ax0.set_title('Pitch & Note Frequency')\n",
        "    ax0.legend()\n",
        "\n",
        "    # Subplot 2: spectral flatness & transition score\n",
        "    ax1 = axes[1]\n",
        "    tone_vals = [row.get('tone_to_noise', 0.0) for row in time_matrix_small]\n",
        "    trans_vals = [row.get('transition_score', 0.0) for row in time_matrix_small]\n",
        "    ax1.plot(times_small, tone_vals, label='Spectral Flatness', color='g')\n",
        "    ax1.plot(times_small, trans_vals, label='Transition Score', color='r')\n",
        "    ax1.set_ylabel('Arbitrary scale')\n",
        "    ax1.set_title('Spectral Flatness & Transition Score')\n",
        "    ax1.set_ylim([0,1])\n",
        "    ax1.legend()\n",
        "\n",
        "    # Subplot 3: praat hnr\n",
        "    ax2 = axes[2]\n",
        "    praat_times = [row.get('start_time_s', 0.0) for row in praat_matrix]\n",
        "    praat_hnr = [row.get('praat_hnr', 0.0) for row in praat_matrix]\n",
        "    ax2.plot(praat_times, praat_hnr, label='Praat HNR (dB)', color='purple')\n",
        "    ax2.set_xlabel('Time (s)')\n",
        "    ax2.set_ylabel('HNR (dB)')\n",
        "    ax2.set_title('Praat HNR (2048-chunk)')\n",
        "    ax2.set_ylim([0,30])\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Figure 2: advanced features + RMS & LUFS + multi-chunk tempo\n",
        "    if audio_data is not None and sr is not None:\n",
        "        n_fft_global = min(len(audio_data), FRAME_SIZE)\n",
        "\n",
        "        fig2, axes2 = plt.subplots(3, 1, figsize=(12,10))\n",
        "\n",
        "        # 1) Log-frequency spectrogram\n",
        "        D = librosa.stft(y=audio_data, n_fft=n_fft_global, hop_length=HOP_LENGTH)\n",
        "        D_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "        axes2[0].set_title('Log-frequency Spectrogram (Advanced)')\n",
        "        img0 = librosa.display.specshow(D_db, sr=sr, hop_length=HOP_LENGTH,\n",
        "                                        x_axis='time', y_axis='log', ax=axes2[0])\n",
        "        fig2.colorbar(img0, ax=axes2[0], format='%+2.0f dB')\n",
        "\n",
        "        # 2) MFCC(60)\n",
        "        mfcc_data = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=60,\n",
        "                                         n_fft=n_fft_global, hop_length=HOP_LENGTH)\n",
        "        axes2[1].set_title('MFCC(60)')\n",
        "        img1 = librosa.display.specshow(mfcc_data, sr=sr, hop_length=HOP_LENGTH,\n",
        "                                        x_axis='time', ax=axes2[1])\n",
        "        fig2.colorbar(img1, ax=axes2[1])\n",
        "\n",
        "        # 3) Chromagram\n",
        "        chroma_data = librosa.feature.chroma_stft(y=audio_data, sr=sr,\n",
        "                                                 n_fft=n_fft_global, hop_length=HOP_LENGTH)\n",
        "        axes2[2].set_title('Chromagram')\n",
        "        img2 = librosa.display.specshow(chroma_data, sr=sr, hop_length=HOP_LENGTH,\n",
        "                                        x_axis='time', y_axis='chroma', ax=axes2[2])\n",
        "        fig2.colorbar(img2, ax=axes2[2])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Figure 3: chunk-based lines for advanced features: ZCR, centroid, rolloff, bandwidth,\n",
        "        # plus RMS(dB)/LUFS and tempo.\n",
        "        times_small_ = [row.get('time_s', 0.0) for row in time_matrix_small]\n",
        "        zcr_vals = [row.get('zcr', 0.0) for row in time_matrix_small]\n",
        "        cent_vals = [row.get('spec_centroid', 0.0) for row in time_matrix_small]\n",
        "        roll_vals = [row.get('spec_rolloff', 0.0) for row in time_matrix_small]\n",
        "        bw_vals = [row.get('spec_bandwidth', 0.0) for row in time_matrix_small]\n",
        "        rms_vals = [row.get('rms_db', 0.0) for row in time_matrix_small]\n",
        "        lufs_vals = [row.get('lufs', 0.0) for row in time_matrix_small]\n",
        "        tempo_vals_small = [row.get('tempo_bpm', 0.0) for row in time_matrix_small]\n",
        "\n",
        "        fig3, axes3 = plt.subplots(8, 1, figsize=(12,22), sharex=True)\n",
        "\n",
        "        axes3[0].plot(times_small_, zcr_vals, 'b', label='Zero Crossing Rate')\n",
        "        axes3[0].set_title('Zero Crossing Rate')\n",
        "        axes3[0].set_ylabel('ZCR')\n",
        "        axes3[0].legend()\n",
        "\n",
        "        axes3[1].plot(times_small_, cent_vals, 'g', label='Spectral Centroid')\n",
        "        axes3[1].set_title('Spectral Centroid')\n",
        "        axes3[1].set_ylabel('Hz')\n",
        "        axes3[1].legend()\n",
        "\n",
        "        axes3[2].plot(times_small_, roll_vals, 'r', label='Spectral Rolloff')\n",
        "        axes3[2].set_title('Spectral Rolloff')\n",
        "        axes3[2].set_ylabel('Hz')\n",
        "        axes3[2].legend()\n",
        "\n",
        "        axes3[3].plot(times_small_, bw_vals, 'm', label='Spectral Bandwidth')\n",
        "        axes3[3].set_title('Spectral Bandwidth')\n",
        "        axes3[3].set_ylabel('Hz')\n",
        "        axes3[3].legend()\n",
        "\n",
        "        axes3[4].plot(times_small_, rms_vals, 'orange', label='RMS(dB)')\n",
        "        axes3[4].set_title('RMS(dB) with LUF_CHUNK_SIZE')\n",
        "        axes3[4].set_ylabel('dB')\n",
        "        axes3[4].legend()\n",
        "\n",
        "        axes3[5].plot(times_small_, lufs_vals, 'c', label='LUFS')\n",
        "        axes3[5].set_title('LUFS with LUF_CHUNK_SIZE')\n",
        "        axes3[5].set_ylabel('LUFS')\n",
        "        axes3[5].legend()\n",
        "\n",
        "        axes3[6].plot(times_small_, tempo_vals_small, 'k', label='Small-chunk Tempo (512)')\n",
        "        axes3[6].set_title('Small-chunk Tempo (512)')\n",
        "        axes3[6].set_ylabel('BPM')\n",
        "        axes3[6].legend()\n",
        "\n",
        "        # 8th subplot merges medium & large tempo lines if available.\n",
        "        if time_matrix_tempo_medium is not None:\n",
        "            med_times = [row.get('start_time_s', 0.0) for row in time_matrix_tempo_medium]\n",
        "            med_bpm = [row.get('tempo_bpm', 0.0) for row in time_matrix_tempo_medium]\n",
        "            axes3[7].plot(med_times, med_bpm, 'y', label='Medium-chunk Tempo (4096)')\n",
        "        if time_matrix_tempo_large is not None:\n",
        "            large_times = [row.get('start_time_s', 0.0) for row in time_matrix_tempo_large]\n",
        "            large_bpm = [row.get('tempo_bpm', 0.0) for row in time_matrix_tempo_large]\n",
        "            axes3[7].plot(large_times, large_bpm, 'r', label='Large-chunk Tempo (16384)')\n",
        "        axes3[7].set_title('Tempo (4096 & 16384)')\n",
        "        axes3[7].set_ylabel('BPM')\n",
        "        axes3[7].set_xlabel('Time (s)')\n",
        "        axes3[7].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c862fd",
      "metadata": {},
      "source": [
        "## 11.5: Classification Summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "96385aed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 14. Classification Summaries (Global/Track-Level Distribution Counts)\n",
        "from collections import Counter\n",
        "\n",
        "def summarize_classifications(\n",
        "    time_matrix_small,\n",
        "    time_matrix_praat,\n",
        "    time_matrix_tempo_medium,\n",
        "    time_matrix_tempo_large\n",
        "):\n",
        "    \"\"\"\n",
        "    Gathers global classification counts across all relevant 'time_matrix' data.\n",
        "    Returns a dictionary, e.g.:\n",
        "\n",
        "    {\n",
        "      \"pitch_accuracy_category\": {\"perfect\": 12, \"good\": 25, ...},\n",
        "      \"tone_to_noise_category\": {...},\n",
        "      \"transition_category\": {...},\n",
        "      \"rms_db_category\": {...},\n",
        "      \"lufs_category\": {...},\n",
        "      \"tempo_bpm_category_small\": {...},\n",
        "      \"tempo_bpm_category_medium\": {...},\n",
        "      \"tempo_bpm_category_large\": {...},\n",
        "      \"praat_hnr_category\": {...}\n",
        "    }\n",
        "\n",
        "    You can adjust or expand these counts as needed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Helper function to safely gather a given field from a list of dictionaries\n",
        "    def gather_categories(matrix, field_name):\n",
        "        # returns e.g. [\"perfect\", \"good\", \"fair\", ...]\n",
        "        cat_list = []\n",
        "        for row in matrix:\n",
        "            val = row.get(field_name, None)\n",
        "            if val is not None:\n",
        "                cat_list.append(val)\n",
        "        return cat_list\n",
        "\n",
        "    # 1) pitch_accuracy_category from time_matrix_small\n",
        "    pitch_categories = gather_categories(time_matrix_small, \"pitch_accuracy_category\")\n",
        "\n",
        "    # 2) tone_to_noise_category from time_matrix_small\n",
        "    tone_categories = gather_categories(time_matrix_small, \"tone_to_noise_category\")\n",
        "\n",
        "    # 3) transition_category from time_matrix_small\n",
        "    transition_categories = gather_categories(time_matrix_small, \"transition_category\")\n",
        "\n",
        "    # 4) RMS & LUFS from time_matrix_small\n",
        "    rms_categories = gather_categories(time_matrix_small, \"rms_db_category\")\n",
        "    lufs_categories = gather_categories(time_matrix_small, \"lufs_category\")\n",
        "\n",
        "    # 5) tempo BPM categories (512) from time_matrix_small\n",
        "    tempo_small_categories = gather_categories(time_matrix_small, \"tempo_bpm_category\")\n",
        "\n",
        "    # 6) medium-tempo matrix\n",
        "    tempo_medium_categories = gather_categories(time_matrix_tempo_medium, \"tempo_bpm_category\")\n",
        "\n",
        "    # 7) large-tempo matrix\n",
        "    tempo_large_categories = gather_categories(time_matrix_tempo_large, \"tempo_bpm_category\")\n",
        "\n",
        "    # 8) Praat HNR\n",
        "    praat_categories = gather_categories(time_matrix_praat, \"praat_hnr_category\")\n",
        "\n",
        "    # Build a dictionary of counts for each classification dimension\n",
        "    classification_summary = {\n",
        "        \"pitch_accuracy_category\": dict(Counter(pitch_categories)),\n",
        "        \"tone_to_noise_category\": dict(Counter(tone_categories)),\n",
        "        \"transition_category\": dict(Counter(transition_categories)),\n",
        "        \"rms_db_category\": dict(Counter(rms_categories)),\n",
        "        \"lufs_category\": dict(Counter(lufs_categories)),\n",
        "        \"tempo_bpm_category_small\": dict(Counter(tempo_small_categories)),\n",
        "        \"tempo_bpm_category_medium\": dict(Counter(tempo_medium_categories)),\n",
        "        \"tempo_bpm_category_large\": dict(Counter(tempo_large_categories)),\n",
        "        \"praat_hnr_category\": dict(Counter(praat_categories))\n",
        "    }\n",
        "\n",
        "    return classification_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod12-classical",
      "metadata": {},
      "source": [
        "## 12. Classical Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "classical-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _replace_nan_with_none(value):\n",
        "    if isinstance(value, float) and np.isnan(value):\n",
        "        return None\n",
        "    elif isinstance(value, list):\n",
        "        return [_replace_nan_with_none(v) for v in value]\n",
        "    elif isinstance(value, tuple):\n",
        "        return tuple(_replace_nan_with_none(v) for v in value)\n",
        "    elif isinstance(value, dict):\n",
        "        return {k: _replace_nan_with_none(v) for k, v in value.items()}\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "def grade_single_file(file_name):\n",
        "    \"\"\"\n",
        "    1) Preprocess => audio_data\n",
        "    2) Noise reduction => clean_audio\n",
        "    3) pitch => times, pitches\n",
        "    4) map pitch => note_names/freqs/deviations\n",
        "    5) stats => dev cents\n",
        "    6) build time_matrix_small (512)\n",
        "    7) Module 7 (Revised): analyze_dynamics_module7 => uses LUF_CHUNK_SIZE for RMS & LUFS.\n",
        "    8) analyze => spectral flatness => tone_to_noise, transition_score\n",
        "    9) advanced features => zcr, centroid, rolloff...\n",
        "    10) small-chunk tempo => 512\n",
        "    11) build time_matrix_praat => 2048 => hnr\n",
        "    12) build time_matrix_tempo_medium => 4096, time_matrix_tempo_large => 16384\n",
        "    13) store in DB => analysis_dict, then plot (including RMS & LUFS)\n",
        "    \"\"\"\n",
        "    input_path = os.path.join(INPUT_DIR, file_name)\n",
        "\n",
        "    # 1. Preprocessing\n",
        "    audio_data, sr = preprocess_audio(input_path)\n",
        "\n",
        "    # 2. Noise reduction\n",
        "    clean_audio = simple_noise_reduction(audio_data, sr)\n",
        "\n",
        "    ##############################################\n",
        "    # PATCH 2: Remove Percussion from Clean Audio\n",
        "    ##############################################\n",
        "    harmonic_audio = remove_percussive_components(clean_audio, sr, margin=(1.0, 1.0))\n",
        "    # now to pass this everywhere \n",
        "    \n",
        "\n",
        "    # 3. pitch\n",
        "    times, pitches, confidences = extract_pitch_contour(harmonic_audio, sr)\n",
        "\n",
        "    # 4. map pitch => note\n",
        "    note_names, note_freqs, deviations = map_pitches_to_notes(pitches)\n",
        "\n",
        "    # 5. stats\n",
        "    valid_devs = [d for d in deviations if d is not None]\n",
        "    if len(valid_devs) > 0:\n",
        "        avg_deviation_cents = float(np.mean(np.abs(valid_devs)))\n",
        "        std_deviation_cents = float(np.std(valid_devs))\n",
        "    else:\n",
        "        avg_deviation_cents = 0.0\n",
        "        std_deviation_cents = 0.0\n",
        "\n",
        "    # 6. build time_matrix_small\n",
        "    time_matrix_small = []\n",
        "    for t, p, nf, d, nn in zip(times, pitches, note_freqs, deviations, note_names):\n",
        "        dev_flag_int = 1 if d is not None and abs(d) > DEVIATION_THRESHOLD else 0\n",
        "        time_matrix_small.append({\n",
        "            \"time_s\": float(t),\n",
        "            \"pitch_hz\": p,\n",
        "            \"note_name\": nn,\n",
        "            \"note_freq_hz\": nf,\n",
        "            \"deviation_cents\": d,\n",
        "            \"dev_flag\": dev_flag_int,\n",
        "            ##########################\n",
        "            # NEW: pitch category\n",
        "            ##########################\n",
        "            \"pitch_accuracy_category\": classify_pitch_deviation(d)\n",
        "        })\n",
        "\n",
        "    # 7. DYNAMICS => RMS & LUFS (with bigger chunk => LUF_CHUNK_SIZE)\n",
        "    time_matrix_small, dyn_summary = analyze_dynamics_module7(\n",
        "        harmonic_audio, sr, time_matrix_small\n",
        "    )\n",
        "\n",
        "    # 8. analyze => tone_to_noise = spectral flatness, transition_score\n",
        "    time_matrix_small = analyze_note_transitions(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 9. advanced features => skip RMS here, we do zcr, centroid, rolloff...\n",
        "    time_matrix_small = update_time_matrix_small_with_advanced_features(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 10. small-chunk tempo => 512\n",
        "    time_matrix_small = analyze_tempo_adherence(harmonic_audio, sr, time_matrix_small)\n",
        "\n",
        "    # 11. build time_matrix_praat => 2048-chunk\n",
        "    time_matrix_praat = build_praat_time_matrix(harmonic_audio, sr)\n",
        "    for row in time_matrix_praat:\n",
        "        row[\"praat_hnr_category\"] = classify_praat_hnr(row[\"praat_hnr\"])\n",
        "\n",
        "    # 12. build time_matrix_tempo_medium => 4096, time_matrix_tempo_large => 16384\n",
        "    time_matrix_tempo_medium = build_larger_tempo_matrix(\n",
        "        harmonic_audio, sr, chunk_size=TEMPO_CHUNK_SIZE_MEDIUM, overlap=0.5\n",
        "    )\n",
        "    time_matrix_tempo_large = build_larger_tempo_matrix(\n",
        "        harmonic_audio, sr, chunk_size=TEMPO_CHUNK_SIZE_LARGE, overlap=0.5\n",
        "    )\n",
        "\n",
        "    # gather final stats => from time_matrix_small\n",
        "    transition_scores = [row.get('transition_score', 0.0) for row in time_matrix_small]\n",
        "    tone_to_noise_vals = [row.get('tone_to_noise', 0.0) for row in time_matrix_small]\n",
        "\n",
        "    if tone_to_noise_vals:\n",
        "        avg_tnr = float(np.mean(tone_to_noise_vals))\n",
        "        std_tnr = float(np.std(tone_to_noise_vals))\n",
        "    else:\n",
        "        avg_tnr = 0.0\n",
        "        std_tnr = 0.0\n",
        "\n",
        "    if transition_scores:\n",
        "        avg_transition_score = float(np.mean(transition_scores))\n",
        "        std_transition_score = float(np.std(transition_scores))\n",
        "    else:\n",
        "        avg_transition_score = 0.0\n",
        "        std_transition_score = 0.0\n",
        "\n",
        "    # gather praat HNR stats\n",
        "    praat_hnr_vals = [row['praat_hnr'] for row in time_matrix_praat]\n",
        "    if praat_hnr_vals:\n",
        "        avg_praat_hnr = float(np.mean(praat_hnr_vals))\n",
        "        std_praat_hnr = float(np.std(praat_hnr_vals))\n",
        "    else:\n",
        "        avg_praat_hnr = 0.0\n",
        "        std_praat_hnr = 0.0\n",
        "\n",
        "\n",
        "\n",
        "    analysis_dict = {\n",
        "        \"file_name\": file_name,\n",
        "        \"sample_rate\": sr,\n",
        "        \"results\": {\n",
        "            \"average_dev_cents\": avg_deviation_cents,\n",
        "            \"std_dev_cents\": std_deviation_cents,\n",
        "            \"deviation_threshold\": DEVIATION_THRESHOLD,\n",
        "            \"avg_transition_score\": avg_transition_score,\n",
        "            \"std_transition_score\": std_transition_score,\n",
        "            \"avg_tnr\": avg_tnr,\n",
        "            \"std_tnr\": std_tnr,\n",
        "            \"avg_praat_hnr\": avg_praat_hnr,\n",
        "            \"std_praat_hnr\": std_praat_hnr\n",
        "        },\n",
        "        \"time_matrix_small\": time_matrix_small,\n",
        "        \"time_matrix_praat\": time_matrix_praat,\n",
        "        \"dynamics_summary\": dyn_summary,\n",
        "        \"time_matrix_tempo_medium\": time_matrix_tempo_medium,\n",
        "        \"time_matrix_tempo_large\": time_matrix_tempo_large\n",
        "    }\n",
        "\n",
        "\n",
        "    ########################################\n",
        "    # NEW: Summarize classification counts\n",
        "    ########################################\n",
        "    classification_summary = summarize_classifications(\n",
        "        time_matrix_small=time_matrix_small,\n",
        "        time_matrix_praat=time_matrix_praat,\n",
        "        time_matrix_tempo_medium=time_matrix_tempo_medium,\n",
        "        time_matrix_tempo_large=time_matrix_tempo_large\n",
        "    )\n",
        "    analysis_dict[\"classification_summary\"] = classification_summary\n",
        "\n",
        "    sanitized_analysis_dict = _replace_nan_with_none(analysis_dict)\n",
        "\n",
        "    db = QuantumMusicDB()\n",
        "    db.create_tables()\n",
        "    rec_id = db.insert_analysis(\n",
        "        file_name=file_name,\n",
        "        sample_rate=sr,\n",
        "        analysis_data=sanitized_analysis_dict\n",
        "    )\n",
        "    print(f\"Inserted analysis record with ID: {rec_id}\")\n",
        "    db.close()\n",
        "\n",
        "    print(\"\\nNow playing the processed audio (bandpass, normalized, trimmed):\")\n",
        "    display(Audio(data=clean_audio, rate=sr))\n",
        "\n",
        "    print(\"\\\\nNow playing the processed audio with percussion removed:\")\n",
        "    display(Audio(data=harmonic_audio, rate=sr))\n",
        "\n",
        "\n",
        "    plot_all_metrics(\n",
        "        time_matrix_small,\n",
        "        time_matrix_praat,\n",
        "        audio_data=clean_audio,\n",
        "        sr=sr,\n",
        "        time_matrix_tempo_medium=time_matrix_tempo_medium,\n",
        "        time_matrix_tempo_large=time_matrix_tempo_large\n",
        "    )\n",
        "\n",
        "    return sanitized_analysis_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mod13-quantum",
      "metadata": {},
      "source": [
        "## 13. Quantum Pipeline (Placeholder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "quantum-placeholder-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 13. QUANTUM PIPELINE (PLACEHOLDER)\n",
        "def quantum_pipeline_placeholder(feature_matrix):\n",
        "    print(\"[Quantum Pipeline Placeholder] Feature matrix received.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "execution-example",
      "metadata": {},
      "source": [
        "# Execution Example\n",
        "Below code cell uses `rohan3.wav` from your `input/` folder.\n",
        "You will see an audio player just before the graphs for quick listening.\n",
        "\n",
        "We now have:\n",
        "- **Module 7** using **LUFS_CHUNK_SIZE** (22050 samples => ~0.5s) for RMS & LUFS\n",
        "- Multi-chunk tempo (512, 4096, 16384)\n",
        "- Plots for RMS(dB) and LUFS in the advanced line plot.\n",
        "- **Short-chunk** fixes for spectral feature calls so no warnings from Librosa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "execution-example-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = grade_single_file(\"rohan3.wav\")\n",
        "# result  # Uncomment to see full analysis dictionary"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "Rohan Agarwal"
      }
    ],
    "kernelspec": {
      "display_name": "quantumvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "name": "QuantumMusic_Unified_LargerLUFS",
    "summary": "A multi-module system for analyzing and grading singing performances with LUFS_CHUNK_SIZE for RMS & LUFS, multi-chunk tempo, and short-chunk n_fft fixes."
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
